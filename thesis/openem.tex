\chapter{Open Event Machine}
\label{chapter:openem}
This thesis investigates the suitability of Open Event Machine (OpenEM) for
real-time stream processing. OpenEM is a programming framework for event-driven
multicore applications.

At the time of writing there are no public documentation resources for the Nokia
Solutions and Networks (NSN) implementation of OpenEM other than the
documentation included in the OpenEM source files available at \cite{openempage}
and the introductory slides at \cite{openemintro}. Hardware vendor
implementations of OpenEM exist and may have more complete documentation
available as is the case for the Texas Instruments OpenEM implementation
introduced in section \ref{sec:tiopenem}.

\section{OpenEM Framework}
\label{sec:emframework}
\subsection{Overview}
OpenEM is an event-driven programming framework originally developed for the
networking data plane by NSN. The OpenEM framework provides a programming model
for scalable and dynamically load balanced applications. The key components of
the OpenEM programming model are events, execution objects, queues and the
scheduler. OpenEM works with run-to-completion principle which means once an
event begins to execute it will not be interrupted but will run until completed.
The run-to-completion principle implies limitations within which well performing
applications must be designed, the main limitations being the required small
granule size for computations and the lock-free implementation of the
application. \cite{openempage}

The OpenEM design principles are explained in the OpenEM source files available
at \cite{openempage}. Many of the OpenEM features described in the following
sections are loosely defined and leave some details unspecified. A selection of
the design principles is presented here to help understand the lack of detail.
The following list is not in any particular order.
\begin{itemize}
    \item OpenEM has been designed to be \emph{easy to implement on different
        multicore SoCs}.
    \item \emph{Easy integration with modern hardware accelerators} is stated
        to have been a major driver in the OpenEM concept design.
    \item \emph{All of the calls in the OpenEM API are multicore safe} meaning
        no data structure gets broken if multiple cores make calls
        simultaneously. However the application designer needs to take the
        parallelism into consideration with regards to the application data
        structures and execution order.
    \item \emph{The API attempts to guide the application designer towards
        portable architecture.}
    \item \emph{The API is not defined for portability through recompilation.}
    \item \emph{OpenEM does not implement a full software platform or a
        middleware solution.} OpenEM implements a driver-level layer of such a
        solution but can be used by the application directly for best
        performance.
\end{itemize}

Another factor explaining the loose definitions is how NSN views the status of
its own public OpenEM implementation targeted for Intel DPDK. The disclaimer in
the NSN source distribution at \cite{openempage} states:
``The implementation of OpenEM for Intel CPUs in this package should
NOT be considered a `Reference OpenEM implementation', but rather an `Example of
an all-SW implementation of OpenEM for Intel CPUs'.'' This helps put the
differences between the OpenEM API specification and the TI OpenEM
implementation introduced in \ref{sec:tiopenem} in context.

The next sections will introduce the key concepts of OpenEM framework as
described in the OpenEM source distribution at \cite{openempage}. The unit of
communication in OpenEM is the Event described in \ref{subsec:event}. Events are
sent to Queues (\ref{subsec:queues}). Queues are connected to Execution Objects
(\ref{subsec:eos}). The scheduler (\ref{subsec:schedule}) chooses an event from
a suitable queue based on scheduling rules and schedules it on a core.

\subsection{Events}
\label{subsec:event}
In the core of the OpenEM framework design is the event. The work to be done by
an OpenEM application is divided into application specific pieces of data
called events. OpenEM does not specify the event content to allow for the use
of hardware specific descriptors in OpenEM implementations for different
hardware platforms. Usually events carry pointers to messages or descriptors.
\cite{openemintro} For example in a network packet processing application an
arriving packet could be converted to or encapsulated by an event.

Event memory is managed by OpenEM, meaning the application allocates events
from OpenEM and freeing events returns the control of the memory to OpenEM
\cite{openemintro}.

OpenEM includes a fork-join helper called \textbf{Event Groups}. Event groups
track the completion of the events sent to the group and send a notification
event once a predetermined number of events have completed. The events belonging
to an event group execute independently of each other and depending on other
scheduling rules may execute on different cores. \cite{openemintro}

\subsection{Execution Objects}
\label{subsec:eos}
The work to be done is described by the events but they don't describe how it
is done. For that purpose there are \textbf{Execution Objects} (EO), which
contain the application logic. The application is built from EOs connected by
queues. Multiple queues can be connected to an EO and an EO may execute on
multiple cores if the queue types of the connected queues allow for it.
\cite{openemintro}

The application logic for each EO is contained in three functions, namely
start, receive and stop, which will get called in different phases of the EO
lifecycle. EO construction and destruction are handled through the application
defined start and stop functions. The receive function will be called whenever
the EO has received an event and is scheduled on a core. \cite{openemintro}

OpenEM implementations will pass a pointer to an user defined context when
calling the receive function. The application designer has complete freedom
over the EO context contents as the OpenEM runtime only passes an user defined
pointer. \cite{openemintro} The application designer should note that as
locking is discouraged and the EO may execute on multiple cores simultaneously,
care must be exercised when accessing the EO context.

\subsection{Queues}
\label{subsec:queues}
The queues attached to the EOs define how the EOs are executed. A queue is
always attached to a single EO but an EO can have multiple queues attached.
Events are sent to queues and scheduled for execution based on queue scheduling
properties: priority, type and queue group. \cite{openemintro}

There are four types of queues: atomic, parallel, parallel ordered and
unscheduled \cite{openemintro}. The programming construct representing the
queues is the same type for all queues but here a queue which type is atomic
will be called an atomic queue for simplicity, this applies to other queues as
well. Events are sent to all queue types the same way using the send function,
the different queue types only affect the scheduling of the events
\cite{openemintro}.

Only one event from an \textbf{atomic queue} may be scheduled at a time. As the
use of locks is discouraged, atomic queues can be used when the application
needs to write shared memory locations. Events from \textbf{parallel queues} can
be scheduled on any core at any time according to other scheduling rules
explained in section \ref{subsec:schedule}. Events received from a
\textbf{parallel ordered queue} are scheduled like events from parallel queues
but OpenEM will restore the event ordering before events are forwarded to other
queues even if the processing of the events ends out of order. The
\textbf{unscheduled queues} are special in that they are not scheduled
automatically. The events are sent to unscheduled queues in the same way they
are sent to the other queue types, but they need to be explicitly dequeued from
the unscheduled queues. Unscheduled queues cannot be added to EOs.
\cite{openempage}

Queues belong to \textbf{queue groups}. Queue groups define the set of cores
the events from the queues in the queue group can be scheduled on. Queue groups
can be used for example to separate application layers, control load balancing
or help guarantee that quality of service or latency targets are reached.
\cite{openemintro} Queue groups can be modified at initialization of the queues
or later during the execution \cite{openempage}. Possibility for modifying the
queue groups during execution can be used to implement dynamic load balancing
in the application.

\subsection{Scheduling}
\label{subsec:schedule}
OpenEM does not define queue scheduling disciplines in detail
\cite{openempage}. The \textbf{scheduler} is implemented in hardware or
software and is specific to the OpenEM implementation. The purpose of the
scheduler across the OpenEM implementations is to dequeue events from the
queues according to its scheduling rules. \cite{openemintro} Queue types and
queue groups affect the scheduling as described in \ref{subsec:queues}.

Each core executes the dispatcher which will be called by the scheduler
whenever an event is scheduled on the core. The dispatcher can run either in an
OS thread or on bare metal. Dispatcher checks which queue the event was
dequeued from and calls the receive function of the EO associated with that
queue. \cite{openemintro}

\subsection{Error Handling}
\label{subsec:error}

\subsection{An Illustrative Example}
\label{subsec:example}
TODO: Present a graphical representation of a simple OpenEM application\\
TODO: Explain the execution of the application

\section[Texas Instruments Implementation of OpenEM]{Texas Instruments
Implementation \\ of OpenEM}
\label{sec:tiopenem}
This section describes the Texas Instruments OpenEM implementation version
1.0.0.2. The TI OpenEM library is delivered as a part of the TI Multicore
Software Development Kit (MCSDK), which is available for download at
\cite{mcsdkdown}. MCSDK distribution contains source files for the OpenEM
components defined in the OpenEM API but no source code is available for the
scheduler. The components for which source code is available all execute on the
DSP cores whereas the scheduler executes on a PDSP core.

\subsection{Scheduling}
The TI OpenEM documentation specifies two alternative scheduling modes, namely
synchronous and asynchronous scheduling. This section describes only the
asynchronous scheduling as the synchronous scheduler is not available in the
version 1.0.0.2. The asynchronous scheduler is deployed on the PDSP cores of the
Multicore Navigator described in \ref{subsec:multicorenav}. The synchronous
scheduler is deployed on the c66x cores described in \ref{subsec:c66x}.

The scheduler running on a PDSP core considers four scheduling criteria:
Priority, Atomicity, Locality and Order. Each queue has a priority. The
scheduler will always select events from the queues which have the highest
priority. Queues are either atomic or parallel. The atomic and parallel queues
function as described in \ref{subsec:queues}. Locality criterion means that the
scheduler tries to schedule consecutive events from any given queue on the same
core if possible. The last criterion for scheduling is order. If multiple events
are eligible for scheduling, the event that has been in a queue longest will be
scheduled. \cite{openemwhite}

Unlike the scheduler, the dispatcher is deployed on the c66x cores. The
dispatcher checks if there are events scheduled for execution on the core it was
called from. The calls to the dispatcher are non-blocking and are made from the
application code, typically from within a dispatch loop. The dispatcher returns
immediately if no events are available for dispatching. If an event is available
the dispatcher will call the receive function of the EO connected to the queue
the event was received from. \cite{openemwhite}

\subsection{Event Preloading}
OpenEM provides an option for event preloading. For events with the event
preloading enabled the event buffers are moved to local L1/L2 ram by the Packet
DMA engine when the scheduler has scheduled the event. Using preloading results
in fewer write stalls. \cite{openemwhite}

\subsection{Hardware Acceleration in TI OpenEM}
HW Queues, PDSP cores

\subsection{Error Handling}

\subsection{Cache Coherency}
OpenEM white paper explains the caching in detail

\subsection{Programming with TI OpenEM}
Initialization, Example\_0

\subsection{OpenEM Tracing}
TI OpenEM includes a built-in tracing feature that provides useful data about
the runtime behaviour. To enable the trace capabilities of the OpenEM runtime the
programmer has to link the application with a specific trace enabled version of
the runtime library. \cite{openemapi}

The trace API is quite simple. The application has to register a trace handler,
which is a function pointer to a function that takes one
\texttt{ti\_em\_scope\_t} and variable number of other arguments, with the
OpenEM runtime. The trace handler will be called every time the runtime or the
application makes calls to OpenEM functions such as \texttt{em\_alloc},
\texttt{em\_send}, \texttt{ti\_em\_dispatch\_once} etc. \cite{openemapi} The
programmer should note that the handler may be called by multiple cores at
overlapping times and therefore race conditions are possible.

Tracing can be used for example to track the number of events in each queue.
This type of tracking may help debug problems with congestion and many other
types of problems with OpenEM.

\subsection{State of TI OpenEM Implementation}
The TI OpenEM library version 1.0.0.2 does not implement the complete OpenEM API
as specified by the NSN implementation of OpenEM described in
\ref{sec:emframework}. The following listing presents the unimplemented features
as listed in the OpenEM library version 1.0.0.2 release notes
\cite{openemnotes} and the TI OpenEM white paper \cite{openemwhite}.

\begin{itemize}
    \item \textbf{Event Groups},
        The TI implementation of event groups is otherwise complete but it lacks
        \texttt{em\_event\_group\_delete} function.
    \item \textbf{Distributed Scheduling},
        A synchronous scheduler is mentioned in \cite{openemwhite}. In
        synchronous mode the scheduler is distributed on the cores and executes
        synchronously after the end of event processing on that core. The
        distributed scheduling is not implemented. Distributed Scheduling is not
        part of the NSN specification of OpenEM.
    \item \textbf{Co-operative dispatcher},
        The co-operative dispatcher described in \cite{openemwhite} provides
        services for suspending and resuming events. The co-operative dispatcher
        only works with the synchronous scheduler.
    \item \textbf{Execution Object context},
        Execution Object context as described in \ref{subsec:eos} is not
        implemented.
    \item \textbf{Parallel Ordered Queue},
        Parallel Ordered Queue type is not implemented.
    \item \textbf{Unscheduled Queue},
        Unscheduled Queue type is not implemented.
\end{itemize}

In addition to the limitations listed above the Queue Group implementation
appears incomplete in the version 1.0.0.2. Queue Groups can be defined and
modified as described in \ref{subsec:queues} but only one queue group can exist
at any given time. This could be a limitation of the hardware platform but there
is not mention of the limitation in the TI OpenEM implementation documentation
or the header files.
