\chapter{Open Event Machine}
\label{chapter:openem}
This thesis investigates the suitability of Open Event Machine (OpenEM) for
real-time stream processing. OpenEM is a programming framework for event-driven
multicore applications.

At the time of writing there are no public documentation resources for the Nokia
Solutions and Networks (NSN) implementation of OpenEM other than the
documentation included in the OpenEM source files available at \cite{openempage}
and the introductory slides at \cite{openemintro}. Hardware vendor
implementations of OpenEM exist and may have more complete documentation
available as is the case for the Texas Instruments OpenEM implementation
introduced in section \ref{sec:tiopenem}.

\section{OpenEM Framework}
\label{sec:emframework}
\subsection{Overview}
OpenEM is an event-driven programming framework originally developed for the
networking data plane by NSN. The OpenEM framework provides a programming model
for scalable and dynamically load balanced applications. The key components of
the OpenEM programming model are events, execution objects, queues and the
scheduler. OpenEM works with run-to-completion principle which means once an
event begins to execute it will not be interrupted but will run until completed.
The run-to-completion principle implies limitations within which well performing
applications must be designed, the main limitations being the required small
granule size for computations and the lock-free implementation of the
application. \cite{openempage}

The OpenEM design principles are explained in the OpenEM source files available
at \cite{openempage}. Many of the OpenEM features described in the following
sections are loosely defined and leave some details unspecified. A selection of
the design principles is presented here to help understand the lack of detail.
The following list is not in any particular order.
\begin{itemize}
    \item OpenEM has been designed to be \emph{easy to implement on different
        multicore SoCs}.
    \item \emph{Easy integration with modern hardware accelerators} is stated
        to have been a major driver in the OpenEM concept design.
    \item \emph{All of the calls in the OpenEM API are multicore safe} meaning
        no data structure gets broken if multiple cores make calls
        simultaneously. However the application designer needs to take the
        parallelism into consideration with regards to the application data
        structures and execution order.
    \item \emph{The API attempts to guide the application designer towards
        portable architecture.}
    \item \emph{The API is not defined for portability through recompilation.}
    \item \emph{OpenEM does not implement a full software platform or a
        middleware solution.} OpenEM implements a driver-level layer of such a
        solution but can be used by the application directly for best
        performance.
\end{itemize}

Another factor explaining the loose definitions is how NSN views the status of
its own public OpenEM implementation targeted for Intel DPDK. The disclaimer in
the NSN source distribution at \cite{openempage} states:
``The implementation of OpenEM for Intel CPUs in this package should
NOT be considered a `Reference OpenEM implementation', but rather an `Example of
an all-SW implementation of OpenEM for Intel CPUs'.'' This helps put the
differences between the OpenEM API specification and the TI OpenEM
implementation introduced in \ref{sec:tiopenem} in context.

The next sections will introduce the key concepts of OpenEM framework as
described in the OpenEM source distribution at \cite{openempage}. The unit of
communication in OpenEM is the Event described in \ref{subsec:event}. Events are
sent to Queues (\ref{subsec:queues}). Queues are connected to Execution Objects
(\ref{subsec:eos}). The scheduler (\ref{subsec:schedule}) chooses an event from
a suitable queue based on scheduling rules and schedules it on a core.

\subsection{Events}
\label{subsec:event}
In the core of the OpenEM framework design is the event. The work to be done by
an OpenEM application is divided into application specific pieces of data
called events. OpenEM does not specify the event content to allow for the use
of hardware specific descriptors in OpenEM implementations for different
hardware platforms. Usually events carry pointers to messages or descriptors.
\cite{openemintro} For example in a network packet processing application an
arriving packet could be converted to or encapsulated by an event.

Event memory is managed by OpenEM, meaning the application allocates events
from OpenEM and freeing events returns the control of the memory to OpenEM
\cite{openemintro}.

OpenEM includes a fork-join helper called \textbf{Event Groups}. Event groups
track the completion of the events sent to the group and send a notification
event once a predetermined number of events have completed. The events belonging
to an event group execute independently of each other and depending on other
scheduling rules may execute on different cores. \cite{openemintro}
\subsection{Execution Objects}
\label{subsec:eos}
The work to be done is described by the events but they don't describe how it
is done. For that purpose there are \textbf{Execution Objects} (EO), which
contain the application logic. The application is built from EOs connected by
queues.  Multiple queues can be connected to an EO and an EO may execute on
multiple cores if the queue types of the connected queues allow for it.
\cite{openemintro}

The application logic for each EO is contained in three functions, namely
start, receive and stop, which will get called in different phases of the EO
lifecycle. EO construction and destruction are handled through the application
defined start and stop functions. The receive function will be called whenever
the EO has received an event and is scheduled on a core. \cite{openemintro}

OpenEM implementations will pass a pointer to an user defined context when
calling the receive function. The application designer has complete freedom
over the EO context contents as the OpenEM runtime only passes an user defined
pointer. \cite{openemintro} The application designer should note that as
locking is discouraged and the EO may execute on multiple cores simultaneously,
care must be exercised when accessing the EO context.

\subsection{Queues}
\label{subsec:queues}
The queues attached to the EOs define how the EOs are executed. A queue is
always attached to a single EO but an EO can have multiple queues attached.
Events are sent to queues and scheduled for execution based on queue scheduling
properties: priority, type and queue group. \cite{openemintro}

There are for types of queues: atomic, parallel, parallel ordered and
unscheduled \cite{openemintro}. The programming construct representing the
queues is the same type for all queues but here a queue which type is atomic
will be called an atomic queue for simplicity, this applies to other queues as
well. Events are sent to all queue types the same way using the send function,
the different queue types only affect the scheduling of the events
\cite{openemintro}.

Only one event from an \textbf{atomic queue} may be scheduled at a time. As the
use of locks is discouraged, atomic queues can be used when the application
needs to write shared memory locations. Events from \textbf{parallel queues} can
be scheduled on any core at any time according to other scheduling rules
explained in section \ref{subsec:schedule}. Events received from a
\textbf{parallel ordered queue} are scheduled like events from parallel queues
but OpenEM will restore the event ordering before events are forwarded to other
queues even if the processing of the events ends out of order. The
\textbf{unscheduled queues} are special in that they are not scheduled
automatically. The events are sent to unscheduled queues in the same way they
are sent to the other queue types, but they need to be explicitly dequeued from
the unscheduled queues. Unscheduled queues cannot be added to EOs.
\cite{openempage}

Queues belong to \textbf{queue groups}. Queue groups define the set of cores
the events from the queues in the queue group can be scheduled on. Queue groups
can be used for example to separate application layers, control load balancing
or help guarantee that quality of service or latency targets are reached.
\cite{openemintro} Queue groups can be modified at initialization of the queues
or later during the execution \cite{openempage}. Possibility for modifying the
queue groups during execution can help in load balancing the application
dynamically.

\subsection{Scheduling}
\label{subsec:schedule}
OpenEM does not define queue scheduling disciplines in detail
\cite{openempage}. The \textbf{scheduler} is implemented in hardware or
software and is specific to the OpenEM implementation. The purpose of the
scheduler across the OpenEM implementations is to dequeue events from the
queues according to its scheduling rules. \cite{openemintro} Queue types and
queue groups affect the scheduling as described in \ref{subsec:queues}.

Each core executes the dispatcher which will be called by the scheduler
whenever an event is scheduled on the core. The dispatcher can run either in an
OS thread or on bare metal. Dispatcher checks which queue the event was
dequeued from and calls the receive function of the EO associated with that
queue. \cite{openemintro}

\subsection{An Illustrative Example}
\label{subsec:example}
Present a graphical representation of a simple OpenEM application\\
Explain the execution of the application

\section{Texas Instruments Implementation of OpenEM}
\label{sec:tiopenem}
This section describes the Texas Instruments OpenEM implementation version
1.0.0.2. The TI OpenEM library is delivered as a part of the TI Multicore
Software Development Kit (MCSDK), which is available for download at
\cite{mcsdkdown}.

Hardware queues\\

\subsection{OpenEM Tracing}
TI OpenEM includes a built-in tracing feature that provides useful data about
the runtime behaviour. To enable the trace capabilities of the OpenEM runtime the
programmer has to link the application with a specific trace enabled version of
the runtime library. \cite{openemapi}

The trace API is quite simple. The application has to register a trace handler,
which is a function pointer to a function that takes one ti\_em\_scope\_t and
variable number of other arguments, with the OpenEM runtime. The trace handler
will be called every time the runtime or the application makes calls to OpenEM
functions such as em\_alloc, em\_send, ti\_em\_dispatch\_once etc.
\cite{openemapi} The programmer should note that the handler may be called by
multiple cores at overlapping times and therefore race conditions are possible.

Tracing can be used for example to track the number of events in each queue.
This type of tracking may help debug problems with congestion and many other
types of problems with OpenEM.

\subsection{Scheduling}
The TI OpenEM scheduler is running on a separate PDSP core. Unfortunately no
source code or assembly for the scheduler is available in the MCSDK distribution.

\subsection{Event Preloading}
OpenEM provides an option for event preloading. For events with the event
preloading enabled the event buffers are moved to local L1/L2 ram by the Packet
DMA engine when the scheduler has scheduled the event. Using preloading results
in fewer write stalls. \cite{openemwhite}

Initialization of OpenEM\\
Cache Coherency
\subsection{State of TI OpenEM Implementation}
The TI OpenEM library version 1.0.0.2 does not implement the complete OpenEM API
as specified by the NSN implementation of OpenEM described in
\ref{sec:emframework}.

Event Groups\\
Distributed Scheduling\\
Queue Group limitations\\
Execution Object context\\
Parallel Ordered Queue\\


