\chapter [OpenEM Experiments] {Characterizing Open Event Machine Performance
Through Experiments}
\label{chapter:experiments}
The objective of these experiments is to understand the behavior and
performance of the Texas Instruments implementation of Open Event Machine in
realtime stream processing. The objective is achieved by comparing the behavior
of an application implemented using OpenEM to the behavior of a similar
application implemented using a simpler multicore runtime system (PREESM) in the
first part of the experiment. \fixme{The second part of the experiment is the
construction of a simulation model. The performance predictions from the
simulation model will be compared to the performance of the real world
application. The objective of the comparison is to help better understand the
OpenEM platform. do we simulate?}

Both of the experiments are described in a similar manner. First the parameters
and factors of the experiments are explained, second the different measurement
setups are introduced and third the results of the measurements are presented.
The comparison of the PREESM and OpenEM applications is described in the section
\ref{sec:firstexperiment}. The performance of the simulation model is described
in \ref{sec:secondexperiment}.

\fixme{as there are many different experiments discussed here, there should be a
summarizing list in the beginning to help the reader grasp what's going on}

\section{Performance Analysis of Runtime Systems}
\label{sec:perfanalysis}
\fixme{this shouldn't probably be in the experiments chapter}
In this section the measurement and performance comparison of software systems
is explained.

The performance analysis of software systems can be split in to three
categories, which are analytical modeling, simulation and measurements. Jain
\cite{jain1991art} defines analytical modeling as using mathematical models to
abstract the key features of the system under study and using these models to
make predictions of the system. Models used in simulations are based on
mathematical abstractions as well. The key difference between analytical
modeling and simulation is the notion of time used in simulation. Analytical
modeling solves the system state at a fixed point in time, in contrast to
simulators where the system state is computed iteratively at multiple points in
time. Analytical modeling and simulation can be used for explorative study of
systems that do not yet exist, whereas analysis using measurements can not be
performed unless the real world system under study exists. The data the
measurements yield is used as the starting point for analysis of the system
under study. \cite{jain1991art}

The successful comparison of software systems requires meaningful and reasonably
accurate measurements of both of the systems. The measurements are obtained by
monitoring the system while it is being subjected to a particular workload
\cite{jain1991art}. Often the monitored applications are built for the
comparison purpose only and therefore any workload they are subjected to is an
approximation of a real world workload that would be processed by a real world
application. These approximate workloads are called synthetic workloads
\cite{jain1991art}. The use of synthetic workload gives more control over the
test conditions and most importantly makes the experiments repeatable. Workload
selection requires care because the synthetic workload needs to mimic its real
world counterpart with high accuracy. Performance analysis is often conducted to
understand the performance or feasibility of a software component or system that
does not exist yet. If the real world version of the workload does not exist or
is not available for study the selection of a workload that loads the system in
a realistic way is difficult \cite{jain1991art}.

\fixme{Todo: explain parameters, factors, the analysis of the results}

The data for analysis is obtained by measuring the workload on the runtime
system. The software measurement tools are called monitors which can be
implemented both in hardware and software. Monitors are classified to software
monitors, hardware monitors, firmware monitors or hybrid monitors depending on
the implementation level of the monitor. The implementation level of the monitor
corresponds to the level of events that can be measured with it conveniently.
For example hardware monitors can monitor the state of registers and hardware
counters but have difficulties in observing the status of software constructs
such as the execution of functions. The software monitors on the other hand can
be used to monitor the status of software components but gathering information
about the status of the hardware is more difficult and in some cases impossible.
\cite{jain1991art} For example it is very complicated to determine whether a
memory operation hit a given level of cache or not using software alone but many
hardware platforms offer hardware counters to monitor the cache hits and misses.

\fixme{needs to have an overview of performance analysis and better explain the
common steps of all methods.}
\fixme{maybe check other sources as well? how about the stuff saarinen uses?
there are also more refs in hanhirova thesis}

\section{Comparison of PREESM and OpenEM Filter Applications}
\label{sec:firstexperiment}
\fixme{ordering in this section is a bit random and it fails to provide a good
idea of what to expect.}
In the first experiment the OpenEM and PREESM filter applications were loaded
with similar workloads and their execution was measured. The idea of the first
experiment is to examine the dynamic scheduling capabilities of the OpenEM
scheduler and the overhead of the OpenEM framework in stream processing. The
OpenEM scheduler is hardware accelerated, running on a separate processor in the
TMS320C6678 chip. The scheduling is explained in detail in chapter
\ref{chapter:openem}. To achieve this objective the OpenEM filter application
introduced in chapter \ref{chapter:construction} was measured under different
loads and compared to a similar application implemented using PREESM. The
PREESM application should be considered a baseline, which demonstrates how a
statically scheduled application behaves under dynamic workload.

The static schedule of the PREESM application was regenerated between every
measurement setup due to the limitations of the code generation in PREESM
framework. The specific limitation was that the parameters of the actor model
were translated into static memory allocations in the code generator, and
manually changing the generated allocations would have been complicated and
prone to error. As a result the actors are scheduled slightly differently
between each scenario. The schedules differ in the mapping of actors to cores
but not in the ordering of the actors. The actor ordering is defined by the
dependencies between the actors and availability of computing resources. To
demonstrate the effect of static scheduling, the estimated actor timings of the
PREESM application were not modified when changing the frame size. \fixme{to
demonstrate the effect... is probably not correct here}

In this experiment the applications are loaded with three different workloads
and measured. In addition the OpenEM application is measured under the same load
but different numbers of available cores. The experiment is explained in the
following subsections. In the first subsection the parameters and factors of the
experiment are introduced. Second the different measurement setups are described
and third the results of the experiment are presented.

\subsection{Parameters and Factors}
Dynamic workload conditions are emulated by repeating the measurements with
different factors. To keep things simple the video streams are not dynamically
switched at runtime. The measurement parameters are presented in the following
listing.

\begin{itemize}
    \item \textbf{Video Frame Size} - The workloads are differentiated by
        changing the frame sizes of the video streams.
    \item \textbf{OpenEM Core Masks} - The OpenEM application is measured
        with different core masks of the Execution Objects.
    \item \textbf{Number of Frames Processed Simultaneously} - The OpenEM
        application processes variable number of frames simultaneously, which
        affects the latency and throughput of the application.
\end{itemize}

The different video frame sizes used are presented in table
\ref{tab:cif_frames}. The frame sizes used are selected from among the Common
Intermediate Format frame sizes. \fixme{find a reference for CIF.} YUV video
format is used in the applications, but only the Y channel is processed by the
applications. The Y channel in the YUV frame contains $R_{x} * R_{y}$ bytes
where $R$ is the resolution. In the YUV format used the U and V channels have
reduced bitrates of $\frac{1}{4} * R_{x} * R_{y}$ per frame.

\begin{table}
    \begin{center}
        \begin{tabular}{ c c c }
            Name  & X resolution  & Y resolution \\ \hline
            QCIF  & 176           & 144          \\ \hline
            CIF   & 352           & 288          \\ \hline
            4CIF  & 704           & 576          \\ \hline
        \end{tabular}
        \caption{CIF frame sizes}
        \label{tab:cif_frames}
    \end{center}
\end{table}

\fixme{super detached. explain when and how and why} The OpenEM measurements are
run with different numbers of frames processed simultaneously to examine the
balancing between throughput and latency.

In the second part of this experiment OpenEM core masks are used to limit the
number of cores available to the filter application. The behavior of OpenEM is
examined under limited resources. The core masks in Texas Instruments
implementation of OpenEM are limited so that only one core mask can be active in
the application as discussed in chapter \ref{chapter:openem}, therefore the core
masks always apply to all execution objects of the application. \fixme{the
detail about ti core masks isn't needed here but more info about what is the
experiment going to show would be great.}

\subsection{Measurement Setups}
The filter applications process two video streams simultaneously as described in
chapter \ref{chapter:construction}. One video stream is processed with a sobel
filter and the other is processed with a gaussian filter. The dynamic behavior
of the applications is investigated using different workloads. The workloads
used are presented in the table \ref{tab:preesm_setups}. The purpose of the
different bitrates used for each video stream is to expose the behavior of the
OpenEM scheduler in handling dynamic workloads. The static schedule in the
PREESM application will provide a baseline to reflect the OpenEM performance to,
but again the performance of the applications should not be directly compared
due to the difference in the runtime systems.

In addition to comparing the PREESM and OpenEM applications the OpenEM
application is measured with different core masks to investigate the dynamic
scheduling with different limitations. Both filters of the OpenEM application
are loaded with CIF streams and different numbers of cores are used. The
experiment is run with core masks allowing one to eight cores being used for
processing the streams.

\fixme{how about the initial events?}
\fixme{maybe the ``first experiment'' should be split in to a number of smaller
experiments to make more sense out of this chapter}

\begin{table}
    \begin{center}
        \begin{tabular}{ c c }
            Sobel Resolution & Gauss Resolution \\ \hline
            CIF              & CIF              \\ \hline
            4CIF             & CIF              \\ \hline
            CIF              & 4CIF             \\ \hline
            QCIF             & QCIF             \\ \hline
        \end{tabular}
        \caption{PREESM and OpenEM measurement setups}
        \label{tab:preesm_setups}
    \end{center}
\end{table}

\subsection{Results}
\fixme{Results and discussion on results are both kind of formless blobs of
text commenting on mixed findings from the experiments. calls for better
division of topics and more structure.}

In this subsection first the results of the measurements of the OpenEM and
PREESM applications are presented and next the results of measuring the OpenEM
application limited with core masks are presented. The latency and throughput of
the OpenEM application measurements are summarized in the tables
\ref{tab:oemthrough} and \ref{tab:oemthrough2}. The corresponding summaries for
the PREESM application are presented in the table \ref{tab:preesmthrough} The
summary of latency and throughtput of the OpenEM application with limited number
of cores is presented in table \ref{tab:oemcoremasks}.

The latency of both of the filters is measured from the time the frame is loaded
from the memory to the time the frame is merged after filtering. The throughput
is measured as frames per second processed in total by the application. Since
both of the applications process frames at the same rate from both streams,
there is only one FPS measurement for each of the measurement setups.
\fixme{maybe move this to construction chapter?}

The latencies of the two streams of the PREESM application in all measurement
setups in the table \ref{tab:preesmthrough} are similar. The largest difference
in the latencies is in the case where sobel filter filters a CIF stream and
gauss filter filters a 4CIF stream, where gauss latency is 60\% higher than the
sobel latency. Much larger differences in the latencies are observed with the
OpenEM application with two initial events presented in table
\ref{tab:oemthrough2} where the largest difference in latencies is also measured
between the CIF sobel stream and 4CIF gauss stream. The gauss latency in the
particular OpenEM measurement is 200\% higher than the sobel latency. The other
OpenEM measurements show larger differences as well compared to PREESM.

\newcommand{\head}[2]{\multicolumn{1}{>{\centering\arraybackslash}p{#1}}{#2}}
\begin{table}
    \begin{center}
        \begin{tabular}{ c c c c c }
            \head{1.5cm}{Sobel latency} & \head{1.5cm}{Gauss latency} &
            \head{1.5cm}{FPS} & \head{1.5cm}{Sobel frame} &
            \head{1.5cm}{Gauss frame} \\ \hline
            5,41 & 8,78 & 223 & CIF & 4CIF \\ \hline
            4,65 & 3,54 & 334 & 4CIF & CIF \\ \hline
            2,15 & 2,51 & 668 & CIF & CIF \\ \hline
            0,61 & 0,71 & 2004 & QCIF & QCIF \\ \hline
        \end{tabular}
        \caption{PREESM latency and throughput. The the latencies are measured
        in milliseconds.}
        \label{tab:preesmthrough}
    \end{center}
\end{table}
\begin{table}
    \begin{center}
        \begin{tabular}{ c c c c c }
            \head{1.5cm}{Sobel latency} & \head{1.5cm}{Gauss latency} &
            \head{1.5cm}{FPS} & \head{1.5cm}{Sobel frame} &
            \head{1.5cm}{Gauss frame} \\ \hline
            3,59 & 10,93 & 256 & CIF & 4CIF \\ \hline
            3,75 & 2,95 & 527 & 4CIF & CIF \\ \hline
            1,42 & 2,80 & 889 & CIF & CIF \\ \hline
            0,32 & 0,71 & 3534 & QCIF & QCIF \\ \hline
        \end{tabular}
        \caption{OpenEM latency and throughput with 2 frames processed
        simultaneously. The latencies are measured in milliseconds.}
        \label{tab:oemthrough2}
    \end{center}
\end{table}
\begin{table}
    \begin{center}
        \begin{tabular}{ c c c c c }
            \head{1.5cm}{Sobel latency} & \head{1.5cm}{Gauss latency} &
            \head{1.5cm}{FPS} & \head{1.5cm}{Sobel frame} &
            \head{1.5cm}{Gauss frame} \\ \hline
            15,82 & 22,85 & 599 & CIF & 4CIF \\ \hline
            4,85 & 3,67 & 895 & 4CIF & CIF \\ \hline
            4,91 & 5,96 & 1955 & CIF & CIF \\ \hline
            1,33 & 1,62 & 7819 & QCIF & QCIF \\ \hline
        \end{tabular}
        \caption{OpenEM latency and throughput with 16 frames processed
        simultaneously. The latencies are measured in milliseconds.}
        \label{tab:oemthrough}
    \end{center}
\end{table}

The PREESM latencies in the table \ref{tab:preesmthrough} are consistently
smaller than the latencies of the throughput optimized OpenEM application in the
table \ref{tab:oemthrough}. The throughput versus latency balance in the OpenEM
application is controlled through the number of frames processed simultaneously.
In this set of measurements the OpenEM application is configured to process 16
frames simultaneously to maximize throughput. \fixme{why is this discussed only
here? how about moving this info to construction?} The effect of increasing the
number of simultaneous frames was examined by measuring the application with
two CIF streams using 2 to 24 initial events. The results of the measurements
are presented in table \ref{tab:oeminitialframes}. The latencies get worse and
the throughput gets better when the number of frames processed simultaneously
increases. This behavior is visualized in figures \ref{fig:oeminitialframesfps}
and \ref{fig:oeminitialframeslat}. The throughput grows rapidly up to 8
simultaneous frames after which the growth slows down. The latencies grow with
each added frame steadily.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/simultaneous_frames_fps.eps}
        \caption{FPS as a function of simultaneous frames.}
        \label{fig:oeminitialframesfps}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/simultaneous_frames_latency.eps}
        \caption{Latencies as functions of simultaneous frames. The latencies
        are measured in millisecods.}
        \label{fig:oeminitialframeslat}
    \end{subfigure}
    \caption{The effect of increasing the number of frames processed
    simultaneously is presented in the figures. The throughput increases rapdily
    up to 8 simultaneous frames after which the growth slows down. The latency grows
    more steadily across all measured setups.}
\end{figure}

The throughput of the application is determined by how much time the application
spends processing the streams versus the time spent doing something else. For
example the synchronization of all cores before each repetition of the PREESM
schedule consumes a lot of cpu cycles as is readily observed from the figure
\ref{fig:preesmcif}. The portion of the bars marked as busy corresponds to the
cycles spent in the synchronization between the repetitions of the schedule. The
percentage of cycles spent in the synchronization varies from 16\% on Core 7
to 45\% on Core 0. The OpenEM dynamic scheduler seems to spread out the work
more evenly in this case where the total overhead cycles are approximately 60\%
for all cores. The core utilization per function is presented in figure
\ref{fig:oem8corefunc}. Most of the overhead cycles in the OpenEM application
are spent waiting for more frames for processing.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/preesm_cifcif.eps}
        \caption{PREESM}
        \label{fig:preesmcif}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/openem_cifcif_2initial_func.eps}
        \caption{OpenEM}
        \label{fig:oem8corefunc}
    \end{subfigure}
    \caption{In the PREESM graph the busy portion of the bars correspond to the
        cycles spent synchronizing the cores between the repetitions of the
        block schedule. The overhead corresponds to cycles spent outside the
        measured functions and the synchronization. In the OpenEM application
        the overhead cycles are the cycles spent outside the compared
    functions.}
\end{figure}

The overhead portions in the figures \ref{fig:preesmcif} and
\ref{fig:oem8corefunc} contain all of the data copying from buffer to buffer
outside the measured functions, but they also contain different amounts of
cycles spent in communications between the cores. The measured functions are the
functions which are explicitly called in the PREESM actor model. Other functions
not included in the measured functions contain runtime specific communication
and some copying of buffers.

\begin{table}
    \begin{center}
        \begin{tabular}{ c c c c }
            \head{1.5cm}{Sobel latency} & \head{1.5cm}{Gauss latency} &
            \head{1.5cm}{FPS} & \head{1.5cm}{Number of cores} \\
            \hline
            57,05 & 57,11 & 263 & 1 \\ \hline
            22,59 & 23,15 & 510 & 2 \\ \hline
            15,09 & 15,84 & 768 & 3 \\ \hline
            10,91 & 11,85 & 1014 & 4 \\ \hline
            8,41 & 9,53 & 1268 & 5 \\ \hline
            7,05 & 8,07 & 1500 & 6 \\ \hline
            5,74 & 6,83 & 1731 & 7 \\ \hline
            4,91 & 5,96 & 1955 & 8 \\ \hline
        \end{tabular}
        \caption{OpenEM measurements with number of cores varied}
        \label{tab:oemcoremasks}
    \end{center}
\end{table}

The second part of this experiment was to limit the number of cores available
for the OpenEM application. The resulting latencies and throughputs are
presented in the table \ref{tab:oemcoremasks}. The increase of the throughput of
the OpenEM application is presented in graph \ref{fig:fpsvcores}. The actual FPS
measured with eight cores is approximately 90\% of the linear growth.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.7\textwidth]{images/coremask_fps.eps}
        \caption{FPS increase as the function of cores vs. linear growth}
        \label{fig:fpsvcores}
    \end{center}
\end{figure}

\begin{table}
    \begin{center}
        \begin{tabular}{ c c c c }
            \head{1.5cm}{Sobel latency} & \head{1.5cm}{Gauss latency} &
            \head{1.5cm}{FPS} & \head{1.5cm}{Simultaneous Frames} \\
            \hline
            1,42  &  2,80  &  889   &  2 \\ \hline
            1,72  &  3,22  &  1371  &  4 \\ \hline
            2,26  &  3,59  &  1655  &  6 \\ \hline
            2,67  &  3,96  &  1825  &  8 \\ \hline
            3,13  &  4,37  &  1880  &  10 \\ \hline
            3,68  &  4,89  &  1921  &  12 \\ \hline
            4,27  &  5,40  &  1940  &  14 \\ \hline
            4,73  &  5,88  &  1958  &  16 \\ \hline
            5,43  &  6,52  &  1976  &  18 \\ \hline
            6,22  &  7,21  &  1983  &  20 \\ \hline
            6,83  &  7,80  &  1981  &  22 \\ \hline
            7,41  &  8,45  &  1992  &  24 \\ \hline
        \end{tabular}
        \caption{OpenEM measurements with different numbers of frames in
        processed simultaneously.}
        \label{tab:oeminitialframes}
    \end{center}
\end{table}

\subsection{Discussion on Results}
This section discusses the results of the first experiment. \fixme{This is
probably not part of the results chapter.}

A block schedule could be generated, which would yield a higher throughput by
introducing multiple instances of the actors for each repetition of the
schedule. The successive repetitions of the schedule would be interleaved and
the overhead of synchronizing the cores would be reduced. The current version of
PREESM does not support the described interleaving of the repetitions
\cite{pelcat2014preesm}.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/openem_cifcif_8cores_eo.eps}
        \caption{Sobel CIF, Gauss CIF}
        \label{fig:oem8coreeo}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/openem_sobel4cif_gausscif_eo.eps}
        \caption{Sobel 4CIF, Gauss Cif}
        \label{fig:oem8coreeosobel4cif}
    \end{subfigure}
    \caption{The bottleneck forming due to the atomic read operation can be
    observed by comparing the core utilization when both of the streams are at
    CIF resolution to the case where sobel resolution is increased to 4CIF.}
\end{figure}

In the table \ref{tab:oemthrough} an improvement of latencies is observed when
the workload is made heavier by moving 4CIF stream from gauss filter to the
sobel filter while the other stream is held at CIF resolution. The probable
cause of the improvement of the latencies when the workload is made heavier by
increasing the sobel stream resolution is the reduced interleaving of the
processing of the subsequent frames. The reduction in the interleaving is caused
by the read execution object which is only connected to an atomic queue. When
the frame size of the sobel stream is increased the read EO starts limiting the
throughput of the application. Fewer frames are processed in parallel which
decreases the time from reading each individual frame to the completion of that
frame. The formation of the bottleneck can be observed by comparing the core
utilization in the figure \ref{fig:oem8coreeo} to the core utilization in the
figure \ref{fig:oem8coreeosobel4cif} where seven cores need to wait for the read
operations on the Core 2 and the overall overhead is increased. The other cores
do not receive any events from the OpenEM scheduler while waiting.

When the gauss stream is increased to 4CIF and the sobel stream is kept at CIF
the bottleneck does not form because computing the gaussian filter for the 4CIF
frames is consuming approximately 80\% of the cycles on all cores. In this case
the read EO only consumes approximately 5\% to 13\% of the cycles on all cores.
Compared to the case of two CIF streams, the latencies in this case are
increased by factors of approximately 3 and 4 for sobel and gauss
correspondingly. The resulting core utilization graph is presented in figure
\ref{fig:oem8coreeogauss4cif}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{images/openem_sobelcif_gauss4cif_eo.eps}
        \caption{OpenEM cycles spent per execution object for CIF sobel frames
        and 4CIF gauss frames}
        \label{fig:oem8coreeogauss4cif}
    \end{center}
\end{figure}

\section{Comparison of Simulated OpenEM Performance and Real OpenEM Performance}
\label{sec:secondexperiment}
\fixme{remove?}
\subsection{Parameters and Factors}
\subsection{Measurement Setups}
\subsection{Results}

