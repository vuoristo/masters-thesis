\FloatBarrier
The first experiment where the throughput and latency of OpenEM were compared to those of PREESM showed that the runtime systems are roughly comparable. The statically scheduled application created with PREESM potentially had the advantage here, since with static schedule no cycles are wasted at any point in the execution in deciding which actor to execute on which core next. OpenEM achieved very similar latencies with the dynamic scheduler. This suggests that the OpenEM scheduler utilizes the hardware resources efficiently and causes relatively small overhead.

The PREESM authors state that PREESM is a prototyping tool~\cite{preesm}, which suggests that it does not necessarily aim to create latency or throughput optimized applications. The gantt chart in figure~\ref{fig:preesm_gantt} suggests that manually optimizing the schedule, a lower latency and higher throughput could be achievable. This is supported by the core utilization graph in figure~\ref{fig:preesmcif} where a large portion of the execution time is spent waiting for other cores. Nevertheless, the OpenEM scheduling performance can be considered good as it roughly matches the statically generated schedule.

In the second experiment the balance of latency versus throughput with the OpenEM scheduler was investigated. The experiment where the number of simultaneously processed frames was varied suggested that by configuring the application the balance between throughput and latency can be controlled. The adjustability of the scheduler is a useful feature to have in real applications, as the throughput can be increased for tasks, which are less latency sensitive without modifying the program code. The extent of control the application designer has depends on the complexity of the application. The application measured in the experiments was simple and it is possible that the results overemphasize the adaptivity compared to more complex applications.

The third experiment was conducted to understand the performance improvement gained from making more processing units available for the runtime. The FPS improvement from one to eight cores is close to 90\% of linear improvement. This means that the sequential portion of the application was small and the scheduler was able to utilize the increased parallelism efficiently.

Since two frames are processed simultaneously the execution of the serial part of the program performed in the read and merge execution objects is interleaved with the execution of the filter execution object of the other frame. This behavior seems to yield a large latency decrease from execution on one core to execution on two cores. Unfortunately this complicates the analysis, since the proportion of the serial and parallel parts of the program is ambiguous. This complication prevents useful comparisons with the theoretical improvement defined by Amdahl's law~\cite{amdahl1967validity}.

The result of the parallel scheduling experiment was not surprising but it is reassuring of the OpenEM scheduler performance. As the total number of events in circulation was quite small compared to the example applications, the scheduler should have no trouble keeping up with the pace of the application even with eight cores.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/openem_cifcif_8cores_eo.eps}
        \caption{Sobel CIF, Gauss CIF}
        \label{fig:oem8coreeo}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/openem_sobel4cif_gausscif_eo.eps}
        \caption{Sobel 4CIF, Gauss Cif}
        \label{fig:oem8coreeosobel4cif}
    \end{subfigure}
    \caption{The bottleneck forming due to the atomic read operation can be observed by comparing the core utilization when both of the streams are at CIF resolution to the case where sobel resolution is increased to 4CIF. In these graphs the application is processing 16 simultaneous events.}
\end{figure}

The throughput optimized application processing 16 events simultaneously exhibits formation of a bottleneck in the atomic execution object. In the table \ref{tab:oemthrough} an improvement of latencies is observed when the workload is made heavier by moving 4CIF stream from gauss filter to the sobel filter while the other stream is kept at CIF resolution.

The probable cause of the improvement of the latencies when the workload is made heavier by increasing the sobel stream resolution is the reduced interleaving of the processing of the subsequent frames. The reduction in the interleaving is caused by the read execution object which is only connected to an atomic queue. When the frame size of the sobel stream is increased the read EO starts limiting the throughput of the application. Fewer frames are processed in parallel which decreases the time from reading each individual frame to the completion of that frame.

The formation of the bottleneck can be observed by comparing the core utilization in the figure \ref{fig:oem8coreeo} to the core utilization in the figure \ref{fig:oem8coreeosobel4cif} where seven cores need to wait for the read operations on the Core 2 and the overall overhead is increased. The other cores do not receive any events from the OpenEM scheduler while waiting.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{images/openem_sobelcif_gauss4cif_eo.eps}
        \caption{OpenEM cycles spent per execution object for CIF sobel frames
        and 4CIF gauss frames}
        \label{fig:oem8coreeogauss4cif}
    \end{center}
\end{figure}

When the gauss stream is increased to 4CIF and the sobel stream is kept at CIF the bottleneck does not form. This is likely because computing the gaussian filter for the 4CIF frames is consuming approximately 80\% of the cycles on all cores. In this case the read EO only consumes approximately 5\% to 13\% of the cycles on all cores. Compared to the case of two CIF streams, the latencies in this case are increased by factors of approximately 3 and 4 for sobel and gauss correspondingly. The resulting core utilization graph is presented in figure \ref{fig:oem8coreeogauss4cif}.
\FloatBarrier
