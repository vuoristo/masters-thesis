\FloatBarrier
By conducting the three experiments described in this chapter the performance of OpenEM in stream processing was investigated. The first experiment where the throughput and latency of OpenEM were compared to those of PREESM showed that the runtime systems are roughly comparable. This suggests that the dynamic scheduler of OpenEM utilizes the hardware resources efficiently and causes relatively small overhead.

In the second experiment the adaptivity of the OpenEM scheduler was investigated. The second experiment where the number of simultaneously processed frames was varied suggested that by configuring the application the balance between throughput and latency can be controlled. The extent of control the application designer has depends on the complexity of the application. The application measured in the experiments was simple and it is possible that the results overemphasize the adaptivity compared to more complex applications.

The third experiment was conducted to understand the performance improvement gained from making more processing units available for the runtime. The FPS improvement from one to eight cores is close to 90\% of linear improvement, which would suggest the overhead of the scheduler is very small. Since two frames are processed simultaneously the execution of the serial part of the program performed in the read and merge execution objects is interleaved with the execution of the filter execution object of the other frame. This behavior seems to yield a large latency decrease from execution on one core to execution on two cores. Unfortunately this complicates the analysis, since the proportion of the serial and parallel parts of the program is ambiguous. This complication prevents useful comparisons with the theoretical improvement defined by Amdahl's law~\cite{amdahl1967validity}.
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/openem_cifcif_8cores_eo.eps}
        \caption{Sobel CIF, Gauss CIF}
        \label{fig:oem8coreeo}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{images/openem_sobel4cif_gausscif_eo.eps}
        \caption{Sobel 4CIF, Gauss Cif}
        \label{fig:oem8coreeosobel4cif}
    \end{subfigure}
    \caption{The bottleneck forming due to the atomic read operation can be observed by comparing the core utilization when both of the streams are at CIF resolution to the case where sobel resolution is increased to 4CIF. In these graphs the application is processing 16 simultaneous events.}
\end{figure}

The throughput optimized application processing 16 events simultaneously exhibits formation of a bottleneck in the atomic execution object. In the table \ref{tab:oemthrough} an improvement of latencies is observed when the workload is made heavier by moving 4CIF stream from gauss filter to the sobel filter while the other stream is kept at CIF resolution.

The probable cause of the improvement of the latencies when the workload is made heavier by increasing the sobel stream resolution is the reduced interleaving of the processing of the subsequent frames. The reduction in the interleaving is caused by the read execution object which is only connected to an atomic queue. When the frame size of the sobel stream is increased the read EO starts limiting the throughput of the application. Fewer frames are processed in parallel which decreases the time from reading each individual frame to the completion of that frame.

The formation of the bottleneck can be observed by comparing the core utilization in the figure \ref{fig:oem8coreeo} to the core utilization in the figure \ref{fig:oem8coreeosobel4cif} where seven cores need to wait for the read operations on the Core 2 and the overall overhead is increased. The other cores do not receive any events from the OpenEM scheduler while waiting.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{images/openem_sobelcif_gauss4cif_eo.eps}
        \caption{OpenEM cycles spent per execution object for CIF sobel frames
        and 4CIF gauss frames}
        \label{fig:oem8coreeogauss4cif}
    \end{center}
\end{figure}

When the gauss stream is increased to 4CIF and the sobel stream is kept at CIF the bottleneck does not form. This is likely because computing the gaussian filter for the 4CIF frames is consuming approximately 80\% of the cycles on all cores. In this case the read EO only consumes approximately 5\% to 13\% of the cycles on all cores. Compared to the case of two CIF streams, the latencies in this case are increased by factors of approximately 3 and 4 for sobel and gauss correspondingly. The resulting core utilization graph is presented in figure \ref{fig:oem8coreeogauss4cif}.
\FloatBarrier
