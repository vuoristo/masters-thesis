Texas Instruments (TI) provides an implementation of the OpenEM framework for its multi-core digital signal processors. The OpenEM programming model is suitable for developing DSP applications that utilize the multiple cores of the DSP as well as the various accelerators available. There are many multicore runtime systems available for multi-core DSPs, including embedded operating systems and operating system agnostic frameworks such as OpenMP. According to TI the motivation for introducing OpenEM for their multi-core DSPs is that the existing options are not designed for heterogeneous platforms and lack support for features that are often required in DSP applications such as real time constraints~\cite{openemwhite}

The TI OpenEM library is delivered as a part of the TI Multicore Software Development Kit (MCSDK), which is available for download at \cite{mcsdkdown}. MCSDK distribution contains source files for the OpenEM components defined in the OpenEM API but no source code is available for the scheduler. In the TI implementation of OpenEM all of the cores run an identical runtime component, which implements most of the framework functionality. The scheduler is not part not part of this component. The scheduler is deployed on a separate PDSP core.

This section describes the Texas Instruments OpenEM implementation version 1.0.0.2. The features specific to the TI implementation of the OpenEM framework are presented in the subsections \ref{subsec:ti-scheduling} Scheduling, \ref{subsec:ti-preloading} Event Preloading, \ref{subsec:ti-acceleration} Hardware Acceleration, \ref{subsec:ti-cache-coherency} Cache Coherency and \ref{subsec:ti-tracing} Tracing. After the descriptions of the features, a look at programming with the TI OpenEM framework is taken in \ref{subsec:ti-init-layer}. Finally the state of the TI OpenEM implementation is examined in \ref{subsec:ti-implementation-state}.

\subsection{Scheduling}
\label{subsec:ti-scheduling}
The TI OpenEM implementation follows the scheduling rules defined by the general OpenEM queue properties as explained in \ref{subsec:queues} and \ref{subsec:schedule}. In addition to the scheduling criteria defined by the OpenEM framework, execution locality is considered by the TI OpenEM scheduler. Locality criterion means that the scheduler tries to schedule events so that the cores can execute the same execution object consecutively as much as possible.~\cite{openemwhite} In DSP applications context switching between execution objects may become expensive if for example the program cache does not fit both the execution object ending its execution on the core and the execution object to replace it. By considering locality the scheduler may decrease the overhead caused by the context switching.

The TI OpenEM documentation specifies two alternative scheduling modes, namely synchronous and asynchronous scheduling. The asynchronous scheduler is deployed on one of the PDSP cores of the Multicore Navigator described in \ref{subsec:multicorenav}. The scheduling operations of the asynchronous scheduler are triggered by scheduling requests sent by the c66x cores described in \ref{subsec:c66x}, but the actual event selection is performed on the PDSP core. The synchronous scheduler is deployed on the c66x cores. The synchronous scheduler interleaves performing the scheduling decisions with executing the execution objects. The synchronous scheduler is not available in the TI OpenEM version 1.0.0.2.~\cite{openemwhite}

The part of the framework responsible for scheduling consists of two parts, the scheduler and the dispatcher as explained in \ref{subsec:schedule}. The dispatcher is deployed on the c66x cores. The dispatcher checks if there are events scheduled for execution on the core it was called from. The calls to the dispatcher are non-blocking and are made from the application code, typically from within a dispatch loop. The dispatcher returns immediately if no events are available for dispatching. If an event is available the dispatcher will call the receive function of the execution object connected to the queue the event was received from.~\cite{openemwhite}

\subsection{Event Pre-loading}
\label{subsec:ti-preloading}
The global event buffers, containing the event payload are located in the shared memory of the device. The buffers can be located in the MSMC RAM or in the DDR RAM depending on the space requirements of the buffers. If the event buffer of an event resides in the shared memory when the event has been dispatched, reading the buffer will have to access the shared memory and this will cause read stalls. In the TI OpenEM implementation these stalls can be minimized by using event pre-loading.~\cite{openemwhite}

Event pre-loading means that moving the event buffers to core local L1 or L2 memory is started before the event has been dispatched. When an event with event pre-loading enabled is scheduled, one of the Packet DMA engines of the Multicore Navigator will begin to move the event buffers to the local memory. Event pre-loading is enabled by the user for every event separately.~\cite{openemwhite}

\subsection{Hardware Acceleration in Texas Instruments OpenEM}
\label{subsec:ti-acceleration}
TI OpenEM utilises Multicore Navigator extensively for offloading the scheduling and inter-core communication from the DSP cores. Multicore Navigator consists of features that enable hardware accelerated communication between the on-chip devices, including hardware queues and separate cores for queue management. It is described more detail in \ref{subsec:multicorenav}.

The main hardware components used by the OpenEM framework apart from the DSP cores and the different types of memory available, are the hardware queues and one of the PDSP cores of the Multicore Navigator. The hardware queues are used for implementing the software queues of the OpenEM framework. The exact mapping of the software queues to the hardware queues is not documented. The asynchronous scheduler is run on one of the PDSP cores. OpenEM does not use all of the Multicore Navigator queues or PDSP cores, thus some of the Multicore Navigator resources are available for the user. TI OpenEM provides a mechanism in the framework initialization to divide the Multicore Navigator resources between the application and the framework.~\cite{openemuser}

In OpenEM applications the computational work is done in the execution objects. The execution objects are always deployed on the DSP cores, but they may distribute parts of the communication on hardware accelerators as well. Deeper interaction of the OpenEM application and the hardware accelerators is possible with certain devices that support interfacing with Multicore Navigator. These hardware devices may produce and consume events without software intervention.~\cite{openemwhite}

The initialization of the OpenEM framework and the required hardware components is handled by the software abstraction layer provided with the example application in~\cite{openemuser}. The software abstraction layer is the best source of information for details about the use of hardware accelerators in OpenEM.

\subsection{Cache Coherency}
\label{subsec:ti-cache-coherency}
The Keystone DSP devices support caching of data in the higher levels of memory to the L1 and L2 local memories but they do not have hardware support for cache coherency \cite{openemapi}. In the general case the application programmer is responsible for managing the cache coherency of the application. TI OpenEM provides optional automatic cache coherency management for event buffers. The cache coherency mode can be set for each event individually. Automatic coherency management is also partially supported for queue contexts.~\cite{openemwhite}

\subsection{OpenEM Tracing}
\label{subsec:ti-tracing}
TI OpenEM includes a built-in tracing feature that provides useful data about the runtime behaviour. To enable the trace capabilities of the OpenEM runtime the programmer has to link the application with a specific trace enabled version of the runtime library. \cite{openemapi}

\fixme{don't list the openem function names etc here, but rather explain what's going on} The trace API is quite simple. The application has to register a trace handler, which is a function pointer to a function that takes one \texttt{ti\_em\_scope\_t} and variable number of other arguments, with the OpenEM runtime. The trace handler will be called every time the runtime or the application makes calls to OpenEM functions such as \texttt{em\_alloc}, \texttt{em\_send}, \texttt{ti\_em\_dispatch\_once} etc. \cite{openemapi} The programmer should note that the handler may be called by multiple cores at overlapping times and therefore race conditions are possible.

Tracing can be used for example to track the number of events in each queue. This type of tracking may help debug problems with congestion and many other types of problems with OpenEM.

\subsection{Programming with TI OpenEM}
\label{subsec:ti-init-layer}
\fixme{Initialization, Example\_0}

\subsection{State of TI OpenEM Implementation}
\label{subsec:ti-implementation-state}
The TI OpenEM library version 1.0.0.2 does not implement the complete OpenEM API as specified by the NSN implementation of OpenEM described in \ref{sec:emframework}. The following listing presents the unimplemented features as listed in the OpenEM library version 1.0.0.2 release notes \cite{openemnotes} and the TI OpenEM white paper \cite{openemwhite}.

\begin{itemize}
    \item \textbf{Event Groups},
        The TI implementation of event groups is otherwise complete but it lacks \texttt{em\_event\_group\_delete} function.
    \item \textbf{Distributed Scheduling},
        A synchronous scheduler is mentioned in \cite{openemwhite}. In synchronous mode the scheduler is distributed on the cores and executes synchronously after the end of event processing on that core. The distributed scheduling is not implemented. Distributed Scheduling is not part of the NSN specification of OpenEM.
    \item \textbf{Co-operative dispatcher},
        The co-operative dispatcher described in \cite{openemwhite} provides services for suspending and resuming events. The co-operative dispatcher only works with the synchronous scheduler.
    \item \textbf{Execution Object context},
        Execution Object context as described in \ref{subsec:eos} is not implemented.
    \item \textbf{Parallel Ordered Queue},
        Parallel Ordered Queue type is not implemented.
    \item \textbf{Unscheduled Queue},
        Unscheduled Queue type is not implemented.
\end{itemize}

In addition to the limitations listed above the Queue Group implementation appears incomplete in the version 1.0.0.2. Queue Groups can be defined and modified as described in \ref{subsec:queues} but only one queue group can exist at any given time. This could be a limitation of the hardware platform but there is no mention of the limitation in the TI OpenEM implementation documentation or the header files.

