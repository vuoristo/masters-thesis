Texas Instruments (TI) provides an implementation of the OpenEM framework for its multi-core digital signal processors. The OpenEM programming model is suitable for developing DSP applications that utilize the multiple cores of the DSP as well as the various accelerators available. There are many multicore runtime systems available for multi-core DSPs, including embedded operating systems and operating system agnostic frameworks such as OpenMP. According to TI the motivation for introducing OpenEM for their multi-core DSPs is that the existing options are not designed for heterogeneous platforms and lack support for features that are often required in DSP applications such as real time constraints~\cite{moerman2014open}.

The TI OpenEM library is delivered as a part of the TI Multicore Software Development Kit (MCSDK), which is available for download at \cite{mcsdkdown}. MCSDK distribution contains source files for the OpenEM components defined in the OpenEM API but no source code is available for the scheduler. In the TI implementation of OpenEM all of the cores run an identical runtime component, which implements most of the framework functionality. The scheduler is not part not part of this component. The scheduler is deployed on a separate PDSP core.

This section describes the Texas Instruments OpenEM implementation version 1.0.0.2. The features specific to the TI implementation of the OpenEM framework are presented in the subsections \ref{subsec:ti-scheduling} Scheduling, \ref{subsec:ti-preloading} Event Preloading, \ref{subsec:ti-acceleration} Hardware Acceleration, \ref{subsec:ti-cache-coherency} Cache Coherency and \ref{subsec:ti-tracing} Tracing. After the descriptions of the features, a look at programming with the TI OpenEM framework is taken in \ref{subsec:ti-init-layer}. Finally the state of the TI OpenEM implementation is examined in \ref{subsec:ti-implementation-state}.

\subsection{Scheduling}
\label{subsec:ti-scheduling}
The TI OpenEM implementation follows the scheduling rules defined by the general OpenEM queue properties as explained in \ref{subsec:queues} and \ref{subsec:schedule}. In addition to the scheduling criteria defined by the OpenEM framework, execution locality is considered by the TI OpenEM scheduler. Locality criterion means that the scheduler tries to schedule events so that the cores can execute the same execution object consecutively as much as possible.~\cite{moerman2014open} In DSP applications context switching between execution objects may become expensive if for example the program cache does not fit both the execution object ending its execution on the core and the execution object to replace it. By considering locality the scheduler may decrease the overhead caused by the context switching.

The TI OpenEM documentation specifies two alternative scheduling modes, namely synchronous and asynchronous scheduling. The asynchronous scheduler is deployed on one of the PDSP cores of the Multicore Navigator described in \ref{subsec:multicorenav}. The scheduling operations of the asynchronous scheduler are triggered by scheduling requests sent by the c66x cores described in \ref{subsec:c66x}, but the actual event selection is performed on the PDSP core. The synchronous scheduler is deployed on the c66x cores. The synchronous scheduler interleaves performing the scheduling decisions with executing the execution objects. The synchronous scheduler is not available in the TI OpenEM version 1.0.0.2.~\cite{moerman2014open}

The part of the framework responsible for scheduling consists of two parts, the scheduler and the dispatcher as explained in \ref{subsec:schedule}. The dispatcher is deployed on the c66x cores. The dispatcher checks if there are events scheduled for execution on the core it was called from. The calls to the dispatcher are non-blocking and are made from the application code, typically from within a dispatch loop. The dispatcher returns immediately if no events are available for dispatching. If an event is available the dispatcher will call the receive function of the execution object connected to the queue the event was received from.~\cite{moerman2014open}

\subsection{Event Pre-loading}
\label{subsec:ti-preloading}
The global event buffers, containing the event payload are located in the shared memory of the device. The buffers can be located in the MSMC RAM or in the DDR RAM depending on the space requirements of the buffers. If the event buffer of an event resides in the shared memory when the event has been dispatched, reading the buffer will have to access the shared memory and this will cause read stalls. In the TI OpenEM implementation these stalls can be minimized by using event pre-loading.~\cite{moerman2014open}

Event pre-loading means that moving the event buffers to core local L1 or L2 memory is started before the event has been dispatched. When an event with event pre-loading enabled is scheduled, one of the Packet DMA engines of the Multicore Navigator will begin to move the event buffers to the local memory. Event pre-loading is enabled by the user for every event separately.~\cite{moerman2014open}

\subsection{Hardware Acceleration in Texas Instruments OpenEM}
\label{subsec:ti-acceleration}
TI OpenEM utilises Multicore Navigator extensively for offloading the scheduling and inter-core communication from the DSP cores. Multicore Navigator consists of features that enable hardware accelerated communication between the on-chip devices, including hardware queues and separate cores for queue management. It is described more detail in \ref{subsec:multicorenav}.

The main hardware components used by the OpenEM framework apart from the DSP cores and the different types of memory available, are the hardware queues and one of the PDSP cores of the Multicore Navigator. The hardware queues are used for implementing the software queues of the OpenEM framework. The exact mapping of the software queues to the hardware queues is not documented. The asynchronous scheduler is run on one of the PDSP cores. OpenEM does not use all of the Multicore Navigator queues or PDSP cores, thus some of the Multicore Navigator resources are available for the user. TI OpenEM provides a mechanism in the framework initialization to divide the Multicore Navigator resources between the application and the framework.~\cite{openemuser}

In OpenEM applications the computational work is done in the execution objects. The execution objects are always deployed on the DSP cores, but they may distribute parts of the communication on hardware accelerators as well. Deeper interaction of the OpenEM application and the hardware accelerators is possible with certain devices that support interfacing with Multicore Navigator. These hardware devices may produce and consume events without software intervention.~\cite{moerman2014open}

The initialization of the OpenEM framework and the required hardware components is handled by the software abstraction layer provided with the example application in~\cite{openemuser}. The software abstraction layer is the best source of information for details about the use of hardware accelerators in OpenEM.

\subsection{Cache Coherency}
\label{subsec:ti-cache-coherency}
The Keystone DSP devices support caching of data in the higher levels of memory to the L1 and L2 local memories but they do not have hardware support for cache coherency \cite{openemapi}. In the general case the application programmer is responsible for managing the cache coherency of the application. TI OpenEM provides optional automatic cache coherency management for event buffers. The cache coherency mode can be set for each event individually. Automatic coherency management is also partially supported for queue contexts.~\cite{moerman2014open}

\subsection{OpenEM Tracing}
\label{subsec:ti-tracing}
TI OpenEM includes a built-in tracing feature that provides data about the runtime behaviour. The trace API is simple to use. The application has to register a trace handler with the OpenEM runtime and link the application with a trace enabled version of the runtime library.~\cite{openemapi}

The trace handler will be called every time the runtime or the application makes calls to OpenEM functions and passed information about the type of the call made. The programmer should note that the handler may be called by multiple cores at overlapping times and therefore race conditions are possible.~\cite{openemapi} Tracing can be used for example to track the number of events in each queue. This type of tracking may help debug problems with congestion and many other types of problems with OpenEM.

\subsection{Programming with TI OpenEM}
\label{subsec:ti-init-layer}
The OpenEM framework was not designed by NSN to be a full software platform or a middleware solution as explained in the OpenEM design principles available at \cite{openempage}, which are reproduced in \ref{sec:emframework}. However the level of abstraction is suitable for DSP programming, which is done close to the hardware without an operating system. This means that the developer using TI OpenEM for developing applications for DSPs has to pay attention to the hardware initialization, memory allocation and cache coherency for their own code as well as for the framework.~\cite{openemuser}

The hardware initialization required by TI OpenEM is implemented in the example application described in \cite{openemuser}. The initialization code is not a part of the OpenEM framework, but the initialization code provided in the example application works without modification or with minor modifications for most purposes. The initialization layer provides an API for co-ordinating the hardware resource use between the application and the framework. \cite{openemuser} The framework initialization documentation is incomplete and the application developer has to look at the source code of the example initialization layer to find out about the resource use of the framework if there exists a possibility of conflicting use.

\subsection{State of TI OpenEM Implementation}
\label{subsec:ti-implementation-state}
The TI OpenEM library version 1.0.0.2 does not implement the complete OpenEM API as specified by the NSN implementation of OpenEM described in \ref{sec:emframework}. The following listing presents the unimplemented features as listed in the OpenEM library version 1.0.0.2 release notes \cite{openemnotes} and the TI OpenEM white paper \cite{moerman2014open}.

\begin{itemize}
    \item \textbf{Event Groups},
        The TI implementation of event groups is functional but lacks the functionality to delete event groups.
    \item \textbf{Distributed Scheduling},
        A synchronous scheduler is described in \cite{moerman2014open}. Distributed Scheduling is not part of the NSN specification of OpenEM.
    \item \textbf{Co-operative dispatcher},
        The co-operative dispatcher is described in \cite{moerman2014open} provides services for suspending and resuming events. The co-operative dispatcher only works with the synchronous scheduler.
    \item \textbf{Execution Object context},
    \item \textbf{Parallel Ordered Queue},
    \item \textbf{Unscheduled Queue},
\end{itemize}

In addition to the limitations listed above the Queue Group implementation appears incomplete in the version 1.0.0.2. Queue Groups can be defined and modified as described in \ref{subsec:queues} but only one queue group can exist at a time. This could be a limitation of the hardware platform but there is no mention of the limitation in the TI OpenEM implementation documentation or the header files.

