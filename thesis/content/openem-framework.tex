OpenEM is an event-driven programming framework originally developed for the networking data plane by NSN. The OpenEM framework provides a programming model for scalable and dynamically load balanced applications. The key components of the OpenEM programming model are events, execution objects, queues and the scheduler. OpenEM operates with so-called run-to-completion principle. The run-to-completion principle means that once an event starts executing it will not be interrupted. New events cannot be scheduled until a core has completed its current task even if the events in execution had lower priorities than the events forced to wait. This implies limitations within which well performing applications must be designed. The program has to be divided to small events, so that the scheduler may efficiently distribute the computation and adhere to scheduling rules and priorities. Another limitation implied by the run-to-completion principle is that application needs to be implemented lock-free for good performance. \cite{openempage}

The OpenEM framework structure and functionality are loosely defined by the source code distribution available at \cite{openempage}. Apart from the source distribution, there exists no public declaration of the structure and functionality of the framework. This lack of detail is explained by the OpenEM design principles, which are stated in the OpenEM source files at \cite{openempage}. A selection of the design principles is presented here. The following list is not in any particular order.

\begin{itemize}
    \item OpenEM has been designed to be \emph{easy to implement on different multicore SoCs}.
    \item \emph{Easy integration with modern hardware accelerators} is stated to have been a major driver in the OpenEM concept design.
    \item \emph{All of the calls in the OpenEM API are multicore safe} meaning no data structure gets broken if multiple cores make calls simultaneously. However the application designer needs to take the parallelism into consideration with regards to the application data structures and execution order.
    \item \emph{The API attempts to guide the application designer towards portable architecture.}
    \item \emph{The API is not defined for portability through recompilation.}
    \item \emph{OpenEM does not implement a full software platform or a middleware solution.} OpenEM implements a driver-level layer of such a solution but can be used by the application directly for best performance.
\end{itemize}

Another factor explaining the loose definitions is how NSN views the status of its own public OpenEM implementation targeted for Intel DPDK. The disclaimer in the NSN source distribution at \cite{openempage} states: ``The implementation of OpenEM for Intel CPUs in this package should NOT be considered a `Reference OpenEM implementation', but rather an `Example of an all-SW implementation of OpenEM for Intel CPUs'.'' This helps put the differences between the OpenEM API specification and the TI OpenEM implementation introduced in \ref{sec:tiopenem} in context.

The next sections will introduce the key concepts of OpenEM framework as described in the OpenEM source distribution at \cite{openempage}. The unit of communication in OpenEM is the Event described in \ref{subsec:event}. Events are sent to Queues (\ref{subsec:queues}). Queues are connected to Execution Objects (\ref{subsec:eos}). The scheduler (\ref{subsec:schedule}) chooses an event from a suitable queue based on scheduling rules and schedules it on a core. Finally, an abstract example of using OpenEM for network packet processing is given in \ref{subsec:example}.

\subsection{Events}
\label{subsec:event}
In the core of the OpenEM framework design is the event. Events are simply pointers to application specific data structures that define the communication between the different parts of the application. The application creates events and tells the framework where the event is to be passed by enqueuing it in a queue. The scheduler observes the state of the queues in the application and schedules execution objects for execution when there are events available in the queues connected to the execution objects.~\cite{openemintro}

OpenEM does not specify the event content to allow for the OpenEM implementations for different hardware platforms to organize the communication in the most efficient way available. The communication may be handled for example by hardware accelerated inter-core communication units or through simple shared memory. Usually events carry pointers to messages but they may also represent tokens for the scheduler with no data payload.~\cite{openemintro} Event memory is managed by OpenEM, meaning the application allocates events from OpenEM and freeing the events returns the control of the memory to the framework \cite{openemintro}.

OpenEM includes a fork-join helper called \textbf{event groups}. Event groups track the completion of the events sent to the group and send a notification event once a predetermined number of events have completed. The events belonging to an event group execute independently of each other and depending on other scheduling rules may execute on different cores. \cite{openemintro}

\subsection{Execution Objects}
\label{subsec:eos}
The \textbf{Execution Objects} contain the application logic. The application is built from execution objects connected by queues. Multiple queues can be connected to an execution object and an execution object may execute on multiple cores if the queueing rules allow. There are multiple possible queue configurations, which allow the execution objects to be executed on multiple cores, the main method is through the use of parallel queues.~\cite{openemintro}

The application logic for each execution object is implemented in three functions, namely start, receive and stop. Execution object construction and destruction are handled in the application defined start and stop functions. An execution object can be scheduled on a core if there are events in the queues attached to it. When an execution object is scheduled for execution, one of the events in the queues attached to it is dequeued and passed to the receive function. Once the execution object has been scheduled on a core, it will run until the receive function returns.~\cite{openemintro}

OpenEM passes a pointer to a user defined context when calling the receive function. The application designer has complete freedom over the execution object context contents as the OpenEM runtime only passes a user defined pointer.~\cite{openemintro} If the execution object is connected to a parallel queue or multiple atomic queues, the execution object may execute on multiple cores simultaneously and thus the execution object context is shared between the cores. This limits the use of execution object context for content that has to be updated by the execution object, as the use of locks may cause the application to deadlock and is thus discouraged.

\subsection{Queues}
\label{subsec:queues}
The communication between the execution objects of an OpenEM application happens using events. Events can also be created outside of the execution objects. The events are sent to queues. The scheduler selects events for execution using the scheduling rules. The scheduling rules of the application are declared by setting the scheduling properties of the queues. The properties of the queues that affect scheduling are queue priorities, queue types and queue groups.~\cite{openemintro}

A queue is always attached to a single execution object and they cannot be attached to anything else but execution objects. The behavior of queues left unattached to execution objects is not defined by the OpenEM framework. An execption to these rules is the unscheduled queue, which cannot be attached to execution objects, nor anything else. An execution object may have one or more queues attached to it.~\cite{openemintro}

There are four types of queues: atomic, parallel, parallel ordered and unscheduled \cite{openemintro}. Only one event from an \textbf{atomic queue} may be scheduled at a time. As the use of locks is discouraged, atomic queues are the preferred way to control the access to shared memory locations. Events from \textbf{parallel queues} can be scheduled on any core at any time according to other scheduling rules explained in section \ref{subsec:schedule}. Events received from a \textbf{parallel ordered queue} are scheduled like events from parallel queues but OpenEM will restore the event ordering before events are forwarded to other queues even if the processing of the events ends out of order. The \textbf{unscheduled queues} are special in that they are not scheduled automatically. The events are sent to unscheduled queues in the same way they are sent to the other queue types, but they need to be explicitly dequeued from the unscheduled queues.~\cite{openempage}

Queues belong to \textbf{queue groups}. Queue groups define the set of cores the events from the queues in the group can be scheduled on. Queue groups can be used for example to separate application layers, to control load balancing or to help guarantee that quality of service or latency targets are reached. \cite{openemintro} Queue groups can be modified at initialization of the queues or later during the execution \cite{openempage}. Possibility for modifying the queue groups during execution can be used to implement dynamic load balancing of the computation.

\subsection{Scheduling}
\label{subsec:schedule}
The purpose of the OpenEM scheduler is to dequeue events from the queues in order defined by the scheduling rules. The scheduling rules are defined by the queue properties described in~\ref{subsec:queues}. The efficiency of the scheduler is an important factor to the overall performance of the OpenEM applications.~\cite{openempage} OpenEM implementations may target hardware platforms, which provide different facilities for the efficient implementation of the scheduler. For example some hardware platforms, such as the TMS320C6678, have hardware accelerated communication facilities for the communication between the cores. To let the OpenEM implementations leverage the hardware as much as possible, the OpenEM API does not specify anything about the scheduler implementation. Thus the schedulers are always implementation specific.

Scheduling an execution object to a core is a two phase process, where the scheduler selects which event to scheduler on which core and the dispatcher passes one event at a time to an execution object. Each core executes the dispatcher. The dispatcher can run either in an OS thread or on bare metal. Dispatcher checks which queue the event was dequeued from and calls the receive function of the execution object associated with that queue.~\cite{openemintro}

\subsection{Error Handling}
\label{subsec:error}
OpenEM provides two mechanisms for handling errors and exceptions: return values and an error handler. Most of the OpenEM API functions return status codes which can be checked by the application. \cite{openempage} Optionally an error handler can be registered with the OpenEM runtime. The error handler is an application specified function which will be called by the OpenEM implementation when an error happens or the application calls \texttt{em\_error}. The error handler can be used to centralize the error management. The application can register a global error handler and additionally one error handler for each EO. If an error occurs in the scope of an EO and there is an error handler registered with that EO, the runtime will call the error handler. If there is no error handler registered with the EO, the runtime will check if there is a global error handler registered. If there is no applicable error handler registered, the runtime just returns an error status code from the API functions. \cite{openempage}

The error handler can modify the return value of the original API call. This can be useful if a centralized error handling is desired. The error handler can free the reserved memory, do the book keeping for the error and return \texttt{EM\_OK} \fixme{don't use openem keywords without introducing them first}, leaving no need for the application code outside the error handler to check for OpenEM errors. \cite{openempage}

\subsection{An Illustrative Example}
\label{subsec:example}
\begin{figure}[h!]
    \begin{center}
        \input{content/openem-example-flow.tex}
        \caption{The OpenEM packet processing application.}
        \label{fig:openem-example-flow}
    \end{center}
\end{figure}
An abstract example of a network packet processing application demonstrating the key capabilities of the OpenEM framework is presented in this subsection. In a network packet processing application network packets are received and transmitted at variable intervals. The example application has to perform some actions for every received packet and decide if the packet has to be forwarded. When a packet is received it is moved into memory accessible by the processor executing the packet processing application and the application is notified of its arrival. The notification is handled by generating an event corresponding to the packet and enqueueing it in the first queue of the application. The event is generated in either hardware or software depending on the OpenEM implementation. A schematic of the queues, execution objects and the connections between them is presented in figure~\ref{fig:openem-example-flow}

The queues of the application may be atomic or parallel. For this example the first queue is parallel. The events in the parallel queue may be scheduled at any core not executing an event currently by the scheduler. The scheduler adheres to the scheduling rules and its optimization parameters, trying to minimize the need to move execution objects from one core to another. The execution object the parallel queue is connected to decrypts the packet contents, which can be done in parallel for many packets at once. After decryption of the packet contents, the execution object decides if the packet needs to be forwarded or dropped. If the packet is forwarded the event is passed forward to a parallel queue connected to another execution object. The second execution object updates the network packet headers and sends it forward. If the packet is dropped the event is freed by the first execution object, otherwise it is freed by the second execution object.

The application keeps statistics about the number of packets decrypted, dropped and forwarded. Since the tasks the statistics are collected for can be done in parallel, the statistics need to be updated in a thread-safe context. To avoid the use of locks in the application code the statistics events are sent to an atomic queue by the first execution object. The atomic queue is connected to the execution object that accesses the shared memory to update the statistics. The statistics events are freed by the update execution object after it finished the update.
