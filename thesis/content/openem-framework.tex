OpenEM is an event-driven programming framework originally developed for the networking data plane by NSN. The OpenEM framework provides a programming model for scalable and dynamically load balanced applications. The key components of the OpenEM programming model are events, execution objects, queues and the scheduler. OpenEM operates with so-called run-to-completion principle. The run-to-completion principle means that once an event starts executing it will not be interrupted. New events cannot be scheduled until a core has completed its current task even if the events in execution had lower priorities than the events forced to wait. This implies limitations within which well performing applications must be designed. The program has to be divided to small events, so that the scheduler may efficiently distribute the computation and adhere to scheduling rules and priorities. Another limitation implied by the run-to-completion principle is that application needs to be implemented lock-free for good performance. \cite{openempage}

The OpenEM framework structure and functionality are loosely defined by the source code distribution available at \cite{openempage}. Apart from the source distribution, there exists no public declaration of the structure and functionality of the framework. This lack of detail is explained by the OpenEM design principles, which are stated in the OpenEM source files at \cite{openempage}. A selection of the design principles is presented here. The following list is not in any particular order.

\begin{itemize}
    \item OpenEM has been designed to be \emph{easy to implement on different multicore SoCs}.
    \item \emph{Easy integration with modern hardware accelerators} is stated to have been a major driver in the OpenEM concept design.
    \item \emph{All of the calls in the OpenEM API are multicore safe} meaning no data structure gets broken if multiple cores make calls simultaneously. However the application designer needs to take the parallelism into consideration with regards to the application data structures and execution order.
    \item \emph{The API attempts to guide the application designer towards portable architecture.}
    \item \emph{The API is not defined for portability through recompilation.}
    \item \emph{OpenEM does not implement a full software platform or a middleware solution.} OpenEM implements a driver-level layer of such a solution but can be used by the application directly for best performance.
\end{itemize}

Another factor explaining the loose definitions is how NSN views the status of its own public OpenEM implementation targeted for Intel DPDK. The disclaimer in the NSN source distribution at \cite{openempage} states: ``The implementation of OpenEM for Intel CPUs in this package should NOT be considered a `Reference OpenEM implementation', but rather an `Example of an all-SW implementation of OpenEM for Intel CPUs'.'' This helps put the differences between the OpenEM API specification and the TI OpenEM implementation introduced in \ref{sec:tiopenem} in context.

The next sections will introduce the key concepts of OpenEM framework as described in the OpenEM source distribution at \cite{openempage}. The unit of communication in OpenEM is the Event described in \ref{subsec:event}. Events are sent to Queues (\ref{subsec:queues}). Queues are connected to Execution Objects (\ref{subsec:eos}). The scheduler (\ref{subsec:schedule}) chooses an event from a suitable queue based on scheduling rules and schedules it on a core. Finally, an abstract example of using OpenEM for network packet processing is given in \ref{subsec:example}.

\subsection{Events}
\label{subsec:event}
In the core of the OpenEM framework design is the event. Events are simply pointers to application specific data structures that define the communication between the different parts of the application. The application creates events and tells the framework where the event is to be passed by enqueuing it in a queue. The framework passes the event to its receiver when the receiving part of the application is scheduled for execution. The receivers are called execution objects that are explained in more detail in \ref{subsec:eos}.~\cite{openemintro}

OpenEM does not specify the event content to allow for the OpenEM implementations for different hardware platforms to organize the communication in the most efficient way available. The communication may be handled for example by hardware accelerated inter core communication units or through simple shared memory. Usually events carry pointers to messages but they may also represent tokens for the scheduler with no data payload.~\cite{openemintro} Event memory is managed by OpenEM, meaning the application allocates events from OpenEM and freeing the events returns the control of the memory to the framework \cite{openemintro}.

OpenEM includes a fork-join helper called \textbf{event groups}. Event groups track the completion of the events sent to the group and send a notification event once a predetermined number of events have completed. The events belonging to an event group execute independently of each other and depending on other scheduling rules may execute on different cores. \cite{openemintro}

\subsection{Execution Objects}
\label{subsec:eos}
The work to be done is described by the events but they don't describe how it is done. For that purpose there are \textbf{Execution Objects} (EO), which contain the application logic. The application is built from EOs connected by queues. Multiple queues can be connected to an EO and an EO may execute on multiple cores if the queue types of the connected queues allow for it \fixme{make the point about queue types explicit. which queue types allow which don't}. \cite{openemintro}

The application logic for each EO is contained in three functions, namely start, receive and stop, which will get called in different phases of the EO lifecycle. EO construction and destruction are handled through the application defined start and stop functions. The receive function will be called whenever the EO has received an event and is scheduled on a core. \cite{openemintro} OpenEM implementations will pass a pointer to an user defined context when calling the receive function. The application designer has complete freedom over the EO context contents as the OpenEM runtime only passes an user defined pointer. \cite{openemintro} The application designer should note that as locking is discouraged and the EO may execute on multiple cores simultaneously, care must be exercised when accessing the EO context.

\subsection{Queues}
\label{subsec:queues}
The queues attached to the EOs define how the EOs are executed. A queue is always attached to a single EO but an EO can have multiple queues attached\fixme{bad wording}. Events are sent to queues and scheduled for execution based on queue scheduling properties: priority, type and queue group. \cite{openemintro} There are four types of queues: atomic, parallel, parallel ordered and unscheduled \cite{openemintro}. The programming construct representing the queues is the same type for all queues\fixme{remove?}. Events are sent to all queue types the same way using the send function, the different queue types only affect the scheduling of the events \cite{openemintro}.

Only one event from an \textbf{atomic queue} may be scheduled at a time. As the use of locks is discouraged, atomic queues can be used when the application needs to write to shared memory locations. Events from \textbf{parallel queues} can be scheduled on any core at any time according to other scheduling rules explained in section \ref{subsec:schedule}. Events received from a \textbf{parallel ordered queue} are scheduled like events from parallel queues but OpenEM will restore the event ordering before events are forwarded to other queues even if the processing of the events ends out of order. The \textbf{unscheduled queues} are special in that they are not scheduled automatically. The events are sent to unscheduled queues in the same way they are sent to the other queue types, but they need to be explicitly dequeued from the unscheduled queues. Unscheduled queues cannot be added to EOs. \cite{openempage}

Queues belong to \textbf{queue groups}. Queue groups define the set of cores the events from the queues in the queue group can be scheduled on. Queue groups can be used for example to separate application layers, control load balancing or help guarantee that quality of service or latency targets are reached. \cite{openemintro} Queue groups can be modified at initialization of the queues or later during the execution \cite{openempage}. Possibility for modifying the queue groups during execution can be used to implement dynamic load balancing in the application.

\subsection{Scheduling}
\label{subsec:schedule}
OpenEM does not define queue scheduling disciplines in detail \cite{openempage}. The \textbf{scheduler} is implemented in hardware or software and is specific to the OpenEM implementation. The purpose of the scheduler across the OpenEM implementations is to dequeue events from the queues according to its scheduling rules. \cite{openemintro} Queue types and queue groups affect the scheduling as described in \ref{subsec:queues}.  

Each core executes the dispatcher which will be called by the scheduler whenever an event is scheduled on the core. The dispatcher can run either in an OS thread or on bare metal. Dispatcher checks which queue the event was dequeued from and calls the receive function of the EO associated with that queue. \cite{openemintro}

\subsection{Error Handling}
\label{subsec:error}
OpenEM provides two mechanisms for handling errors and exceptions: return values and an error handler. Most of the OpenEM API functions return status codes which can be checked by the application. \cite{openempage} Optionally an error handler can be registered with the OpenEM runtime. The error handler is an application specified function which will be called by the OpenEM implementation when an error happens or the application calls \texttt{em\_error}. The error handler can be used to centralize the error management. The application can register a global error handler and additionally one error handler for each EO. If an error occurs in the scope of an EO and there is an error handler registered with that EO, the runtime will call the error handler. If there is no error handler registered with the EO, the runtime will check if there is a global error handler registered. If there is no applicable error handler registered, the runtime just returns an error status code from the API functions. \cite{openempage}

The error handler can modify the return value of the original API call. This can be useful if a centralized error handling is desired. The error handler can free the reserved memory, do the book keeping for the error and return \texttt{EM\_OK} \fixme{don't use openem keywords without introducing them first}, leaving no need for the application code outside the error handler to check for OpenEM errors. \cite{openempage}

\subsection{An Illustrative Example}
\label{subsec:example}
\begin{figure}[h!]
    \begin{center}
        \input{content/openem-example-flow.tex}
        \caption{The OpenEM packet processing application.}
        \label{fig:openem_flow}
    \end{center}
\end{figure}
An abstract example of a network packet processing application demonstrating the key capabilities of the OpenEM framework is presented in this subsection. In a network packet processing application network packets are received and transmitted at variable intervals. The application has to perform some actions for every received packet and decide if the packet has to be forwarded. When a packet is received it is moved into memory accessible by the processor executing the packet processing application and the application is notified of its arrival. The notification is handled by generating an event corresponding to the packet and enqueueing it in the first queue of the application. The event is generated in either hardware or software depending on the OpenEM implementation.

The queues of the application may be atomic or parallel. For this example the first queue is parallel. The events in the parallel queue may be scheduled at any core not executing an event currently by the scheduler. The scheduler adheres to the scheduling rules and its optimization parameters, trying to minimize the need to move execution objects from one core to another. The execution object the parallel queue is connected to decrypts the packet contents, which can be done in parallel for many packets at once. After decryption of the packet contents, the execution object decides if the packet needs to be forwarded or dropped. If the packet is forwarded the event is passed forward to a parallel queue connected to another execution object. The second execution object updates the network packet headers and sends it forward. If the packet is dropped the event is freed by the first execution object, otherwise it is freed by the second execution object.

The application keeps statistics about the number of packets decrypted, dropped and forwarded. Since the tasks the statistics are collected for can be done in parallel, the statistics need to be updated in a thread-safe context. To avoid the use of locks in the application code the statistics events are sent to an atomic queue by the first execution object. The atomic queue is connected to the execution object that accesses the shared memory to update the statistics. The statistics events are freed by the update execution object after it finished the update.
