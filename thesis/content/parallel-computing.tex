The single-thread performance of computer hardware grew rapidly, even exponentially, for many decades. This growth was reflected in software development where many software developers mainly focused on developing single-threaded programs. The main driver of single threaded performance was the increasing clock-speed of the CPUs. These increases in clock-speed slowed down around 2005 and hardware manufacturers turned more of their focus toward multicores. While parallel computing has been researched and practiced for decades, its importance has grown greatly after the proliferation of multicore processors.~\cite{sutter2005free}

The OpenEM model of parallelism is described in detail in the following sections. In this section a closer look is taken into task parallelism~\ref{subsec:task-parallelism}, which is the form of parallelism implemented by the OpenEM framework and event-driven programming~\ref{subsec:event-driven-programming}, which is a model of concurrency used in OpenEM.

\subsection{Task Parallelism}
\label{subsec:task-parallelism}
Task parallelism is a form of parallelism, in which units of work are distributed among multiple cores. The units of work in task parallelism are called tasks. The tasks consist of code and data in contrast to data parallelism where the same work is performed on multiple pieces of data.~\cite{hennessy2011computer} Task parallelism can be flexibly used to construct parallel programs, as the different tasks can consist of work unrelated to each other. This makes task parallelism useful with handling irregular parallelism~\cite{ayguade2009design}. The coupling results in overhead compared to data parallel models, and thus the grain size of computation needs to be sufficiently large for efficient task parallelism~\cite{subhlok1993exploiting}.

Programming frameworks that provide task parallelism generally provide ways to declare dependencies between the tasks. By implementing task dependencies the programmer may avoid having to manually synchronize the threads of execution. The task scheduler is not aware of the contents of the tasks and thus accessing shared memory inside the tasks that do not have the appropriate dependencies leads to data races and acquiring locks may lead to deadlocks. For these reasons dependencies between tasks are the natural way of synchronization in task parallel programs. Introduction of dependencies between the tasks limits the possible orders of execution. With enough dependencies the only possible schedule may be executing the tasks sequentially. Thus the dependencies between the tasks have large impact on the performance of task parallel programs and therefore unnecessary dependencies should be avoided.~\cite{hennessy2011computer}

Examples of task parallel libraries and programming languages include the Grand Central Dispatch by Apple~\cite{sakamoto2012grand}, OpenMP implements tasks from version 3.0 onwards~\cite{ayguade2009design}, Intel Thread Building Blocks~\cite{pheatt2008intel}, Cilk~\cite{blumofe1996cilk}, .Net Parallel Extensions~\cite{leijen2009design} and Habanero-Java~\cite{barik2009habanero}.

\subsection{Event-driven Programming}
\label{subsec:event-driven-programming}
Along with parallelism, concurrency is becoming more and more common in modern computing. Reasons for increasing concurrency in programs are handling asynchronous IO and responding to external events such as user interactions.

In general IO operations consist of three phases. In the first phase CPU sets up the operation by deciding what is read or written and where. The target hardware could be for example the hard drive of the device. In the second phase the CPU waits for the operation to complete and has nothing to do with regards to the particular operation. In the third phase the CPU is notified of the availability of results and begins processing them. Only phases one and three require active processing from the CPU.~\cite{friesen2015asynchronous} IO operations often take long compared to computation on the CPUs. In IO heavy applications multiple IO operations can be started concurrently and their results be processed in the order of completion. Such concurrent waiting increases the CPU utilization as the CPUs are not spending time busy waiting for IO but instead processing the results of completed IO operations.~\cite{dabek2002event}

A common way of enabling concurrency in programs is to split the execution of the program across multiple software threads. Using threads for concurrent programming is convenient because they allow interleaving IO and computation while preserving the appearance of a serial program. A separate thread could be spawned for each IO operation so that a non-blocked thread can execute while the IO thread is waiting. The use of threads has disadvantages; for example they introduce concurrency even to sections of programs where it is not needed. Programming with threads requires explicit synchronization of the threads, which in practice yields data races and deadlocks. Spawning software threads consumes processing cycles and memory making full software threads often heavier than necessary for the task in hand.~\cite{dabek2002event, lee2006problem}

Concurrent processing of IO and UI generated events does not necessitate the spawning of a thread for each operation. Another way of handling concurrent events is by registering an event handler with a central system that keeps track of completed operations. Such central system is commonly called the event loop. The event loop checks completion of operations and calls the registered event handlers when the results are available. For example the Node.js~\cite{tilkov2010node} and Apple's Grand Central Dispatch~\cite{sakamoto2012grand} frameworks make use of such model of concurrency. The event-driven concurrent programming model leaves the mapping of event loops and details of how events are dispatched to its implementations. One or more threads can execute event loops and the events can even be dispatched from one thread to another depending on the implementation.

Dabek et al. argue for the use of events instead of threads to provide concurrency in IO heavy server environments. The benefits of events are that they provide comparable concurrency as threads in concurrent IO programs but are easier to program and tend to yield more stable performance under heavy loads.~\cite{dabek2002event} This along with the other advantages of events in concurrent programming have generated enough interest that many event-driven processing concepts have been developed. Here are some examples. Event-driven runtime with its own programming language called Eve has been developed by Fonseca et al.~\cite{fonseca2014eve}. Node.js~\cite{tilkov2010node} implements event-driven processing. Majority of the UI libraries such as QT~\cite{blanchette2006cpp} are implemented in an event-driven pattern.
