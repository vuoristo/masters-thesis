To understand the behavior and performance of software systems, quantitative data about the system execution is required. Precisely what data is needed depends on the purpose of the analysis. A couple of popular methods for acquiring quantitative data about software systems are introduced in this section. First, an overview to the performance analysis of software systems is provided in the subsection \ref{subsec:analysing-software}. Second, a closer focus to measuring software systems is taken in subsection \ref{subsec:measuring-software}. Finally, the analysis of the acquired data is described in subsection \ref{subsec:data-analysis}.

\subsection{Analysing Software Systems}
\label{subsec:analysing-software}
According to Jain \cite{jain1991art} the performance analysis of software systems can be split into three categories, which are analytical modeling, simulation and measuring of software systems. Analytical modeling and simulation use mathematical models for the analysis of the software systems. Abstract mathematical models of a system can be constructed without access to the system under study. To measure a software system, access to the specific system under study is always required. There are many methods for modeling and simulation of software systems, but in most cases they require less work to implement than the actual system under study. These properties make the analytical modeling and simulation attractive for explorative study of software systems that do not exist yet.~\cite{jain1991art}

The key difference between analytical modeling and simulation is the notion of time present in simulation. Analytical modeling solves the system state at a fixed point in time, in contrast to simulators where the system state is computed iteratively at multiple points in time.~\cite{jain1991art}

Performance analysis is used for many different purposes. For example performance analysis can be used to help choose the best performing hardware platform for certain application, or to explore different configurations of an application. Successful analysis requires careful experiment design. First step to successful performance analysis is method selection. Using simulation or analytical modeling can yield results quickly, but they are not as accurate as measuring the real world system.~\cite{jain1991art}

The execution of a computer program is a complex interaction of hardware and software components and thus the number of parameters of the analysis grows large. Factors are parameters that are varied in the analysis. Factors are selected from among all parameters of the system \cite{jain1991art}. Every factor increases the time it takes to complete the analysis and only few of the parameters are relevant for the result of the analysis. Thus, the factor selection requires clear goals for the analysis and a good understanding of the problem space so that the most relevant factors are chosen. The factor selection of the experiments conducted in this thesis is discussed in section \ref{subsec:factorselection}.

\subsection{Measuring Software Systems}
\label{subsec:measuring-software}
In this thesis a synthetic workload application is constructed and its performance is measured. In this subsection a closer look at measuring software systems is taken.

The successful comparison of software systems requires meaningful and reasonably accurate measurements of the systems under study. Measurements are obtained by monitoring the system while it is being subjected to a particular workload \cite{jain1991art}. Often the monitored applications are built for the comparison purpose only and therefore any workload they are subjected to is an approximation of the real world workload that would be processed by their real world application counterparts. These approximate workloads are called synthetic workloads. The use of a synthetic workload gives more control over the test conditions and most importantly makes the experiments repeatable.~\cite{jain1991art}

Synthetic workload creation requires care because it needs to mimic its real world counterpart with high accuracy to provide useful any information. Performance analysis is often conducted to understand the performance or feasibility of a software component or a system that does not exist yet. In such situations synthetic workloads need to be used out of necessity.  

The workload is the target of the measurements but it does not define the exact measurements that are to be conducted. Selection of metrics is equally important as the selection of the workload for successful analysis. The best metrics for a given analysis are determined by what is the goal of the analysis. \cite{jain1991art} For example measurements can be used for comparison of throughput of two comparable software systems. In that case clearly a good measure of throughput is needed.

The software measurement tools are called monitors, which can be implemented both in hardware and in software. Monitors are classified to software monitors, hardware monitors, firmware monitors or hybrid monitors depending on the implementation level of the monitor. The implementation level of the monitor affects the level of events that are convenient to measure with it. For example hardware monitors can monitor the state of registers and hardware counters but have difficulties in observing the status of software constructs such as the execution of functions. The software monitors on the other hand can be used to monitor the status of software components but gathering information about the status of the hardware is more difficult and in some cases impossible. \cite{jain1991art} For example it is very complicated to determine whether a memory operation hit a given level of cache or not using software alone but many hardware platforms offer hardware counters to monitor the cache hits and misses.

\subsection{Analysis of the Data}
\label{subsec:data-analysis}
The goal of the performance analysis is to get actionable results about the systems under study. The data obtained from the models or measurements is not in itself enough for making well-grounded decisions. The models and measurements may yield millions of values for the observed variables and the analyst needs to decide how to best represent the data so that the phenomena behind the data are explained \cite{jain1991art}.

Statistical methods are used to analyze the numerical data and expose the causation and correlation between the factors and the results. Often in the literature simple statistical tools such as mean, mode and standard deviation are used to represent the data in only a few numbers. Such simple statistics are enough if they capture the relevant information about behavior of the system. For example if the goal of the analysis is to compare the latency of two non-realtime systems, an average of the latency and its standard deviation over a reasonable measurement period can be enough. A more thorough look at the statistical tools is provided in \cite{jain1991art}.

The results of the analysis are often easiest to understand when presented in graphical form. Graphical representations of the data such as histograms, line charts and bar charts are commonly used. These graphs are very generic and used in many fields to present many kinds of data. There are also more domain specific visualizations of data such as the gantt charts used to represent schedules in computer context and elsewhere. The visualizations of data are designed to be faster to understand than the corresponding numerical views to the same data but they have their limitations. The graphical representations are inaccurate and if they are not carefully prepared they may present a biased view to the real data. Due to these limitations the visualizations should be prepared carefully and the numerical data they are based on should be also made available.~\cite{jain1991art}
