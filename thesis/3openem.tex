\chapter{Open Event Machine}
\label{chapter:openem}
This thesis investigates the performance of Open Event Machine (OpenEM) for stream processing. OpenEM is a programming framework for creating multicore applications. It was originally developed by Nokia Solutions and Networks (NSN) for the networking dataplane~\cite{openemintro}. 

In this chapter the OpenEM framework is introduced. To get a good view of the background of the framework, parallel computing is discussed in~\ref{sec:parallel-computing}. The section about parallel computing contains descriptions of task parallelism, which is the form of parallelism implemented by OpenEM and event-driven programming, which is the model of concurrency used in OpenEM. After the context has been introduced two views to the OpenEM framework are provided. First, the OpenEM framework as specified by Nokia is described in \ref{sec:emframework}. Second, the Texas Instruments implementation of the OpenEM framework for TMS320C6678 is explained in~\ref{sec:tiopenem}.

\section{Parallel Computing}
\label{sec:parallel-computing}
The single-thread performance of computer hardware grew rapidly, even exponentially, for many decades. This growth was reflected in software development where many software developers mainly focused on developing single-threaded programs. The main driver of single threaded performance was the increasing clock-speed of the CPUs. These increases in clock-speed slowed down around 2005 and hardware manufacturers turned more of their focus toward multicores. While parallel computing has been researched and practiced for decades, its importance has grown greatly after the proliferation of multicore processors.~\cite{sutter2005free}

The OpenEM model of parallelism is described in detail in the following sections. In this section a closer look is taken into task parallelism~\ref{subsec:task-parallelism}, which is the form of parallelism implemented by the OpenEM framework and event-driven programming~\ref{subsec:event-driven-programming}, which is a model of concurrency used in OpenEM.

\subsection{Task Parallelism}
\label{subsec:task-parallelism}
Task parallelism is a form of parallelism, in which units of work are distributed among multiple cores. The units of work in task parallelism are called tasks. The tasks consist of code and data in contrast to data parallelism where the same work is performed on multiple pieces of data.~\cite{hennessy2011computer} Task parallelism can be flexibly used to construct parallel programs, as the different tasks can consist of work unrelated to each other. This makes task parallelism useful with handling irregular parallelism~\cite{ayguade2009design}. The coupling results in overhead compared to data parallel models, and thus the grain size of computation needs to be sufficiently large for efficient task parallelism~\cite{subhlok1993exploiting}.

Programming frameworks that provide task parallelism generally provide ways to declare dependencies between the tasks. By implementing task dependencies the programmer may avoid having to manually synchronize the threads of execution. The task scheduler is not aware of the contents of the tasks and thus accessing shared memory inside the tasks that do not have the appropriate dependencies leads to data races and acquiring locks may lead to deadlocks. For these reasons dependencies between tasks are the natural way of synchronization in task parallel programs. Introduction of dependencies between the tasks limits the possible orders of execution. With enough dependencies the only possible schedule may be executing the tasks sequentially. Thus the dependencies between the tasks have large impact on the performance of task parallel programs and therefore unnecessary dependencies should be avoided.~\cite{hennessy2011computer}

Examples of task parallel libraries and programming languages include the Grand Central Dispatch by Apple~\cite{sakamoto2012grand}, OpenMP implements tasks from version 3.0 onwards~\cite{ayguade2009design}, Intel Thread Building Blocks~\cite{pheatt2008intel}, Cilk~\cite{blumofe1996cilk}, .Net Parallel Extensions~\cite{leijen2009design} and Habanero-Java~\cite{barik2009habanero}.

\subsection{Event-driven Programming}
\label{subsec:event-driven-programming}
Along with parallelism, concurrency is becoming more and more common in modern computing. Reasons for increasing concurrency in programs are handling asynchronous IO and responding to external events such as user interactions.

In general IO operations consist of three phases. In the first phase CPU sets up the operation by deciding what is read or written and where. The target hardware could be for example the hard drive of the device. In the second phase the CPU waits for the operation to complete and has nothing to do with regards to the particular operation. In the third phase the CPU is notified of the availability of results and begins processing them. Only phases one and three require active processing from the CPU.~\cite{friesen2015asynchronous} IO operations often take long compared to computation on the CPUs. In IO heavy applications multiple IO operations can be started concurrently and their results be processed in the order of completion. Such concurrent waiting increases the CPU utilization as the CPUs are not spending time busy waiting for IO but instead processing the results of completed IO operations.~\cite{dabek2002event}

A common way of enabling concurrency in programs is to split the execution of the program across multiple software threads. Using threads for concurrent programming is convenient because they allow interleaving IO and computation while preserving the appearance of a serial program. A separate thread could be spawned for each IO operation so that a non-blocked thread can execute while the IO thread is waiting. The use of threads has disadvantages; for example they introduce concurrency even to sections of programs where it is not needed. Programming with threads requires explicit synchronization of the threads, which in practice yields data races and deadlocks. Spawning software threads consumes processing cycles and memory making full software threads often heavier than necessary for the task in hand.~\cite{dabek2002event, lee2006problem}

Concurrent processing of IO and UI generated events does not necessitate the spawning of a thread for each operation. Another way of handling concurrent events is by registering an event handler with a central system that keeps track of completed operations. Such central system is commonly called the event loop. The event loop checks completion of operations and calls the registered event handlers when the results are available. For example the Node.js~\cite{tilkov2010node} and Apple's Grand Central Dispatch~\cite{sakamoto2012grand} frameworks make use of such model of concurrency. The event-driven concurrent programming model leaves the mapping of event loops and details of how events are dispatched to its implementations. One or more threads can execute event loops and the events can even be dispatched from one thread to another depending on the implementation.

Dabek et al. argue for the use of events instead of threads to provide concurrency in IO heavy server environments. The benefits of events are that they provide comparable concurrency as threads in concurrent IO programs but are easier to program and tend to yield more stable performance under heavy loads.~\cite{dabek2002event} This along with the other advantages of events in concurrent programming have generated enough interest that many event-driven processing concepts have been developed. Here are some examples. Event-driven runtime with its own programming language called Eve has been developed by Fonseca et al.~\cite{fonseca2014eve}. Node.js~\cite{tilkov2010node} implements event-driven processing. Majority of the UI libraries such as QT~\cite{blanchette2006cpp} are implemented in an event-driven pattern.

\section{OpenEM Framework}
\label{sec:emframework}
The OpenEM framework provides a programming model for scalable and dynamically load balanced applications. OpenEM model of parallelism is task parallelism with the distinct feature that it separates the task code and data to different entities. The OpenEM tasks are scheduled using a dynamic scheduler that queues the events for one or more cores running the event loop. The Texas Instruments implementation of OpenEM uses hardware accelerators for the scheduling and inter-core communication.

The key components of the OpenEM programming model are events, execution objects, queues and the scheduler. OpenEM operates with so-called run-to-completion principle. The run-to-completion principle means that once an event starts executing it will not be interrupted. New events cannot be scheduled until a core has completed its current task even if the events in execution had lower priorities than the events forced to wait. This implies limitations within which well performing applications must be designed. The program has to be divided to small events, so that the scheduler may efficiently distribute the computation and adhere to scheduling rules and priorities. Another limitation implied by the run-to-completion principle is that the application needs to be implemented lock-free for good performance.~\cite{openempage}

The OpenEM framework structure and functionality are loosely defined by the source code distribution available at \cite{openempage}. Apart from the source distribution, there exists no public declaration of the structure and functionality of the framework. This lack of detail is explained by the OpenEM design principles, which are stated in the OpenEM source files at \cite{openempage}. A selection of the design principles is presented here. The following list is not in any particular order.

\begin{itemize}
    \item OpenEM has been designed to be \emph{easy to implement on different multicore SoCs}.
    \item \emph{Easy integration with modern hardware accelerators} is stated to have been a major driver in the OpenEM concept design.
    \item \emph{All of the calls in the OpenEM API are multicore safe} meaning no data structure gets broken if multiple cores make calls simultaneously. However the application designer needs to take the parallelism into consideration with regards to the application data structures and execution order.
    \item \emph{The API attempts to guide the application designer towards portable architecture.}
    \item \emph{The API is not defined for portability through recompilation.}
    \item \emph{OpenEM does not implement a full software platform or a middleware solution.} OpenEM implements a driver-level layer of such a solution but can be used by the application directly for best performance.
\end{itemize}

Another factor explaining the loose definitions is how NSN views the status of its own public OpenEM implementation targeted for Intel DPDK. The disclaimer in the NSN source distribution at \cite{openempage} states: ``The implementation of OpenEM for Intel CPUs in this package should NOT be considered a `reference OpenEM implementation', but rather an `example of an all-SW implementation of OpenEM for Intel CPUs'.'' This helps put the differences between the OpenEM API specification and the TI OpenEM implementation introduced in \ref{sec:tiopenem} in context.

The next sections will introduce the key concepts of OpenEM framework as described in the OpenEM source distribution at \cite{openempage}. The unit of communication in OpenEM is the Event described in \ref{subsec:event}. Events are sent to Queues (\ref{subsec:queues}). Queues are connected to Execution Objects (\ref{subsec:eos}). The scheduler (\ref{subsec:schedule}) chooses an event from a suitable queue based on scheduling rules and schedules it on a core. Finally, an abstract example of using OpenEM for network packet processing is given in \ref{subsec:example}.

\subsection{Events}
\label{subsec:event}
In the core of the OpenEM framework design is the event. Events are simply pointers to application specific data structures that define the communication between the different parts of the application. The application creates events and tells the framework where the event is to be passed by enqueueing it in a queue. The scheduler observes the state of the queues in the application and schedules execution objects for execution when there are events available in the queues connected to the execution objects.~\cite{openemintro}

OpenEM does not specify the event content to allow for the OpenEM implementations for different hardware platforms to organize the communication in the most efficient way available. The communication may be handled for example by hardware accelerated inter-core communication units or through simple shared memory. Usually events carry pointers to messages but they may also represent tokens for the scheduler with no data payload.~\cite{openemintro} Event memory is managed by OpenEM, meaning the application allocates events from OpenEM and freeing the events returns the control of the memory to the framework \cite{openemintro}.

OpenEM includes a fork-join helper called \textbf{event groups}. Event groups track the completion of the events sent to the group and send a notification event once a predetermined number of events have completed. The events belonging to an event group execute independently of each other and depending on other scheduling rules may execute on different cores. \cite{openemintro}

\subsection{Execution Objects}
\label{subsec:eos}
The execution objects contain the application logic. The application is built from execution objects connected by queues. Multiple queues can be connected to an execution object and an execution object may execute on multiple cores if the queueing rules allow. There are multiple possible queue configurations, which allow the execution objects to be executed on multiple cores, the main method is through the use of parallel queues.~\cite{openemintro}

The application logic for each execution object is implemented in three functions, namely start, receive and stop. Execution object construction and destruction are handled in the application defined start and stop functions. An execution object can be scheduled on a core if there are events in the queues attached to it. When an execution object is scheduled for execution, one of the events in the queues attached to it is dequeued and passed to the receive function. Once the execution object has been scheduled on a core, it will run until the receive function returns.~\cite{openemintro}

OpenEM passes a pointer to a user defined context when calling the receive function. The application designer has complete freedom over the execution object context contents as the OpenEM runtime only passes a user defined pointer.~\cite{openemintro} If the execution object is connected to a parallel queue or multiple atomic queues, the execution object may execute on multiple cores simultaneously and thus the execution object context is shared between the cores. This limits the use of execution object context for content that has to be updated by the execution object, as the use of locks may cause the application to deadlock and is thus discouraged.

\subsection{Queues}
\label{subsec:queues}
The communication between the execution objects of an OpenEM application happens using events. The execution objects send events to first in first out queues, which are attached to other execution objects. The scheduler selects which queue to pick events for execution based on queue properties. The properties of the queues that affect scheduling are queue priorities, queue types and queue groups.~\cite{openemintro} The queues define what can be executed in parallel and in what order different execution objects are executed for a given event, thus constituting the high-level schedule of the application.

A queue is always attached to a single execution object and it cannot be attached to anything else. The behavior of queues left unattached to execution objects is not defined in the OpenEM framework. An exception to this is the unscheduled queue, which cannot be attached to execution objects, nor anything else. An execution object may have one or more queues attached to it.~\cite{openemintro}

There are four types of queues: atomic, parallel, parallel ordered and unscheduled \cite{openemintro}. Only one event from an \textbf{atomic queue} may be scheduled at a time. As the use of locks is discouraged, atomic queues are the preferred way to control the access to shared memory locations. Events from \textbf{parallel queues} can be scheduled on any core at any time according to other scheduling rules explained in section \ref{subsec:schedule}. Events received from a \textbf{parallel ordered queue} are scheduled like events from parallel queues but OpenEM will restore the event ordering before events are forwarded to other queues even if the processing of the events ends out of order. The \textbf{unscheduled queues} are special in that they are not scheduled automatically. The events are sent to unscheduled queues in the same way they are sent to the other queue types, but they need to be explicitly dequeued from the unscheduled queues.~\cite{openempage}

Queues belong to \textbf{queue groups}. Queue groups define the set of cores the events from the queues in the group can be scheduled on. Queue groups can be used for example to separate application layers, to control load balancing or to help guarantee that quality of service or latency targets are reached. \cite{openemintro} Queue groups can be modified at initialization of the queues or later during the execution \cite{openempage}. Possibility for modifying the queue groups during execution can be used to implement dynamic load balancing of the computation.

\subsection{Scheduling}
\label{subsec:schedule}
The purpose of the OpenEM scheduler is to choose, which events to execute on which cores. The scheduler does this by dequeueing events from the queues in order defined by the scheduling rules. The scheduling rules are defined by the queue properties described in~\ref{subsec:queues}. The efficiency of the scheduler is an important factor to the overall performance of the OpenEM applications.~\cite{openempage} OpenEM implementations may target hardware platforms, which provide different facilities for the efficient implementation of the scheduler. For example some hardware platforms, such as the TMS320C6678, have hardware accelerated communication facilities for the communication between the cores. To let the OpenEM implementations leverage the hardware as much as possible, the OpenEM API does not specify anything about the scheduler implementation. Thus the schedulers are always implementation specific.

Scheduling an execution object to a core is a two phase process, where the scheduler selects which event to schedule on which core and the dispatcher passes one event at a time to an execution object. Each core executes the dispatcher. The dispatcher can run either in an OS thread or on bare metal. Dispatcher checks which queue the event was dequeued from and calls the receive function of the execution object associated with that queue.~\cite{openemintro}

\subsection{Error Handling}
\label{subsec:error}
OpenEM provides two mechanisms for handling errors and exceptions: return values and an error handler. Most of the OpenEM API functions return status codes which can be checked by the application to implement simple error handling.~\cite{openempage} Optionally an error handler can be registered with the OpenEM runtime.

The error handler is an application specified function, which will be called by the OpenEM implementation when the framework encounters an error or the application explicitly invokes an OpenEM error. The error handler can be used to centralize the error management. The application can register a global error handler and additionally one error handler for each execution object. If an error occurs in the scope of an execution object and there is an error handler registered with it, the runtime will call the error handler. If there is no error handler registered with the execution object, the runtime will check if there is a global error handler registered. If there is no applicable error handler registered, the runtime just returns an error status code from the API function.~\cite{openempage}

The error handler can modify the return value of the original API call. This can be useful if centralized error handling is desired. The error handler can free the reserved memory, do the bookkeeping for the error and return a non-error status code from the original API call, leaving no need for the application code outside the error handler to check for OpenEM errors.~\cite{openempage}

\subsection{An Illustrative Example}
\label{subsec:example}
\begin{figure}[h!]
    \begin{center}
        \input{content/openem-example-flow.tex}
        \caption{The OpenEM packet processing application.}
        \label{fig:openem-example-flow}
    \end{center}
\end{figure}
An abstract example of a network packet processing application demonstrating the key capabilities of the OpenEM framework is presented in this subsection. In a network packet processing application network packets are received and transmitted at variable intervals. The example application has to perform some actions for every received packet and decide if the packet has to be forwarded. When a packet is received it is moved into memory accessible by the processor executing the packet processing application and the application is notified of its arrival. The notification is handled by generating an event corresponding to the packet and enqueueing it in the first queue of the application. The event is generated in either hardware or software depending on the OpenEM implementation. A schematic of the queues, execution objects and the connections between them is presented in figure~\ref{fig:openem-example-flow}

The queues of the application may be atomic or parallel. For this example the first queue is parallel. The events in the parallel queue may be scheduled on any core not executing an event currently. The scheduler adheres to the scheduling rules and its optimization parameters, trying to minimize the need to move execution objects from one core to another. The execution object the parallel queue is connected to decrypts the packet contents, which can be done in parallel for many packets at once. After decryption of the packet contents, the execution object decides if the packet needs to be forwarded or dropped. If the packet is forwarded the event is passed forward to a parallel queue connected to another execution object. The second execution object updates the network packet headers and sends it forward. If the packet is dropped the event is freed by the first execution object, otherwise it is freed by the second execution object.

The application keeps statistics about the number of packets decrypted, dropped and forwarded. Since the tasks the statistics are collected for can be done in parallel, the statistics need to be updated in a thread-safe context. To avoid the use of locks in the application code the statistics events are sent to an atomic queue by the first execution object. The atomic queue is connected to the execution object that accesses the shared memory to update the statistics. The statistics events are freed by the update execution object after it finished the update.

\section[Texas Instruments Implementation of OpenEM]{Texas Instruments Implementation\\of OpenEM}
\label{sec:tiopenem}
Texas Instruments (TI) provides an implementation of the OpenEM framework for its Keystone I family of multicore digital signal processors. The OpenEM programming model is suitable for developing DSP applications that utilize the multiple cores of the DSP as well as the various accelerators available. There are many multicore runtime systems available for multicore DSPs, including embedded operating systems and operating system agnostic frameworks such as OpenMP. According to TI the motivation for introducing OpenEM for their multicore DSPs is that the existing options are not designed for heterogeneous platforms and lack support for features that are often required in DSP applications such as real time constraints~\cite{moerman2014open}.

The TI OpenEM library is delivered as a part of the TI Multicore Software Development Kit (MCSDK), which is available for download at \cite{mcsdkdown}. MCSDK distribution contains source files for the OpenEM components defined in the OpenEM API but no source code is available for the scheduler. In the TI implementation of OpenEM all of the cores run an identical runtime component, which implements most of the framework functionality. The scheduler is not part not part of this component. The scheduler is deployed on a separate PDSP core.

This section describes the Texas Instruments OpenEM implementation version 1.0.0.2. The features specific to the TI implementation of the OpenEM framework are presented in the subsections \ref{subsec:ti-scheduling} Scheduling, \ref{subsec:ti-preloading} Event Preloading, \ref{subsec:ti-acceleration} Hardware Acceleration, \ref{subsec:ti-cache-coherency} Cache Coherency and \ref{subsec:ti-tracing} Tracing. After the descriptions of the features, a look at programming with the TI OpenEM framework is taken in \ref{subsec:ti-init-layer}. Finally the state of the TI OpenEM implementation is examined in \ref{subsec:ti-implementation-state}.

\subsection{Scheduling}
\label{subsec:ti-scheduling}
The TI OpenEM implementation follows the scheduling rules defined by the general OpenEM queue properties as explained in \ref{subsec:queues} and \ref{subsec:schedule}. In addition to the scheduling criteria defined by the OpenEM framework, execution locality is considered by the TI OpenEM scheduler. Locality criterion means that the scheduler tries to schedule events so that the cores can execute the same execution object consecutively as much as possible.~\cite{moerman2014open} In DSP applications context switching between execution objects may become expensive if for example the program cache does not fit both the execution object ending its execution on the core and the execution object to replace it. By considering locality the scheduler may decrease the overhead caused by the context switching.

The Texas Instruments OpenEM documentation specifies two alternative scheduling modes, namely synchronous and asynchronous scheduling. The asynchronous scheduler is deployed on one of the PDSP cores of the Multicore Navigator described in \ref{subsec:multicorenav}. The scheduling operations of the asynchronous scheduler are triggered by scheduling requests sent by the c66x cores described in \ref{subsec:c66x}, but the actual event selection is performed on the PDSP core. The synchronous scheduler is deployed on the c66x cores. The synchronous scheduler interleaves performing the scheduling decisions with executing the execution objects. The synchronous scheduler is not available in the TI OpenEM version 1.0.0.2.~\cite{moerman2014open}

The part of the framework responsible for scheduling consists of two parts, the scheduler and the dispatcher as explained in \ref{subsec:schedule}. The dispatcher is deployed on the c66x cores. The dispatcher checks if there are events scheduled for execution on the core it was called from. The calls to the dispatcher are non-blocking and are made from the application code, typically from within a dispatch loop. The dispatcher returns immediately if no events are available for dispatching. If an event is available the dispatcher will call the receive function of the execution object connected to the queue the event was received from.~\cite{moerman2014open}

\subsection{Event Pre-loading}
\label{subsec:ti-preloading}
The global event buffers, containing the event payload are located in the shared memory of the device. The buffers can be located in the MSMC RAM or in the DDR RAM depending on the space requirements of the buffers. If the event buffer of an event resides in the shared memory when the event has been dispatched, reading the buffer will have to access the shared memory and this will cause read stalls. In the TI OpenEM implementation these stalls can be minimized by using event pre-loading.~\cite{moerman2014open}

Event pre-loading means that moving the event buffers to core local L1 or L2 memory is started before the event has been dispatched. When an event with event pre-loading enabled is scheduled, one of the Packet DMA engines of the Multicore Navigator will begin to move the event buffers to the local memory. Event pre-loading is enabled by the user for every event separately.~\cite{moerman2014open}

\subsection[Hardware Acceleration in Texas Instruments OpenEM]{Hardware Acceleration in Texas \\ Instruments OpenEM}
\label{subsec:ti-acceleration}
TI OpenEM utilises Multicore Navigator extensively for the scheduler and inter-core communication. Multicore Navigator consists of features that enable hardware accelerated communication between the on-chip devices, including hardware queues and separate cores for queue management. It is described more detail in \ref{subsec:multicorenav}.

The main hardware components used by the OpenEM framework apart from the DSP cores and the different types of memory available, are the hardware queues and one of the PDSP cores of the Multicore Navigator. The hardware queues are used for implementing the software queues of the OpenEM framework. The exact mapping of the software queues to the hardware queues is not documented. The asynchronous scheduler is run on one of the PDSP cores. OpenEM does not use all of the Multicore Navigator queues or PDSP cores, thus some of the Multicore Navigator resources are available for the user. TI OpenEM provides a mechanism in the framework initialization to divide the Multicore Navigator resources between the application and the framework.~\cite{openemuser}

In OpenEM applications the computational work is done in the execution objects. The execution objects are always deployed on the DSP cores, but they may distribute parts of the communication on hardware accelerators as well. Deeper interaction of the OpenEM application and the hardware accelerators is possible with certain devices that support interfacing with Multicore Navigator. These hardware devices may produce and consume events without software intervention.~\cite{moerman2014open}

The initialization of the OpenEM framework and the required hardware components is handled by the software abstraction layer provided with the example application in~\cite{openemuser}. The software abstraction layer is the best source of information for details about the use of hardware accelerators in OpenEM.

\subsection{Cache Coherency}
\label{subsec:ti-cache-coherency}
The Keystone DSP devices support caching of data in the higher levels of memory to the L1 and L2 local memories but they do not have hardware support for cache coherency \cite{openemapi}. In the general case the application programmer is responsible for managing the cache coherency of the application. TI OpenEM provides optional automatic cache coherency management for event buffers. The cache coherency mode can be set for each event individually. Automatic coherency management is also partially supported for queue contexts.~\cite{moerman2014open}

\subsection{OpenEM Tracing}
\label{subsec:ti-tracing}
TI OpenEM includes a built-in tracing feature that provides data about the runtime behaviour. The trace API is simple to use. The application has to register a trace handler with the OpenEM runtime and link the application with a trace enabled version of the runtime library.~\cite{openemapi}

The trace handler will be called every time the runtime or the application makes calls to OpenEM functions and passed information about the type of the call made. The programmer should note that the handler may be called by multiple cores at overlapping times and therefore race conditions are possible.~\cite{openemapi} Tracing can be used for example to track the number of events in each queue. This type of tracking may help debug problems with congestion and many other types of problems with OpenEM.

\subsection{Programming with TI OpenEM}
\label{subsec:ti-init-layer}
The OpenEM framework was not designed by NSN to be a full software platform or a middleware solution as explained in the OpenEM design principles available at \cite{openempage}, which are reproduced in \ref{sec:emframework}. However the level of abstraction is suitable for DSP programming, which is done close to the hardware without an operating system. The low abstraction level means that the developer using TI OpenEM for developing applications for DSPs has to pay attention to the hardware initialization, memory allocation and cache coherency for their own code as well as for the framework.~\cite{openemuser}

The hardware initialization required by TI OpenEM is implemented in the example application described in \cite{openemuser}. The initialization code is not a part of the OpenEM framework, but the initialization code provided in the example application works without modification or with minor modifications for most purposes. The initialization layer provides an API for co-ordinating the hardware resource use between the application and the framework. \cite{openemuser} The framework initialization documentation is incomplete and the application developer has to look at the source code of the example initialization layer to find out about the resource use of the framework if there exists a possibility of conflicting use.

\subsection{State of TI OpenEM Implementation}
\label{subsec:ti-implementation-state}
The TI OpenEM library version 1.0.0.2 does not implement the complete OpenEM API as specified by the NSN implementation of OpenEM described in \ref{sec:emframework}. The following listing presents the unimplemented features as listed in the OpenEM library version 1.0.0.2 release notes \cite{openemnotes} and the TI OpenEM white paper \cite{moerman2014open}.

\begin{itemize}
    \item \textbf{Event Groups},
        The TI implementation of event groups is functional but lacks the functionality to delete event groups.
    \item \textbf{Distributed Scheduling},
        A synchronous scheduler is described in \cite{moerman2014open}. Distributed Scheduling is not part of the NSN specification of OpenEM.
    \item \textbf{Co-operative dispatcher},
        The co-operative dispatcher is described in \cite{moerman2014open} provides services for suspending and resuming events. The co-operative dispatcher only works with the synchronous scheduler.
    \item \textbf{Execution Object context},
    \item \textbf{Parallel Ordered Queue},
    \item \textbf{Unscheduled Queue},
\end{itemize}

In addition to the limitations listed above the Queue Group implementation appears incomplete in the version 1.0.0.2. Queue Groups can be defined and modified as described in \ref{subsec:queues} but only one queue group can exist at a time. This could be a limitation of the hardware platform but there is no mention of the limitation in the TI OpenEM implementation documentation or the header files.
