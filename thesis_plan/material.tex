\subsection{Workload}
The program to be analyzed needs to be similar to real-world programs executed on the platform to get generally useful results. Too simple workload wouldn't sufficiently utilize the hardware accelerated scheduler and therefore wouldn't help us understand the memory behavior of the applications and the capabilities of the analysis tools in analyzing hardware accelerated scheduling on a multicore platform.

Care must be exercised in selection of the workload to not implement a too complex load. Too complex load couldn't be analyzed with the analysis tools we are hoping to use.

The workload will consist of two or more simple filters applied to varying number of video streams. Candidates for suitable filters are Sobel, Canny and similar. The workload will be implemented in two versions: 1. Simple actor based implementation built using PREESM which is scheduled statically according to hand derived schedule. 2. OpenEM adaptation of the simple implementation. OpenEM provides dynamic scheduling.

\subsection{PREESM}
PREESM is a research tool for prototyping actor model software on multi-core platforms. PREESM features graphical interface for construction of the actor network, the inter core communication and the code generation for target platform. PREESM supports code generation for TMS320C6678 and provides example implementation for Sobel filter.

\subsection{TMDSEVM6678L with TMS320C6678 multi-core DSP processor}
Hanhirova's {\tt TECHNICAL REPORT TI TMS320C6678 evaluation board} document explains the hardware and related software well. The C6678 belongs to Texas Instruments Keystone family of DSPs.

The TMS320C6678 has eight C6678 - also known as CorePac - processors on it.

The processors are pipelined with the pipeline capable of dispatching 8 instructions every cycle. The processors also have vector processing units for 32, 16, 8 bit instructions.

In the Keystone devices the shared memory controller doesn't maintain memory coherency between the Cores. The coherency is managed by the application running on the devices. For example in [http://link.springer.com/chapter/10.1007\%2F978-3-642-40698-0\_9\#page-1] OpenEM is utilized for the communication needed for memory coherency.

\subsection{Analysis}
The current candidate for analysing the execution is the CToolsLib Common Platform Tracers (CPTLib) tool which outputs the assembly instructions in order executed by the program and calculates cycle counts for them.

PSE - more info after the PSE introduction on week 23.

The workload execution will be measured dynamically using performance counters available in the TI hardware. The performance counters can be accessed through CPTLib.

\subsection{Multicore programming}
The TMS320C6678 has multiple different models of inter-core communication: Explicit through Inter Processor Communication, Multicore navigator and SRIO (Serial RapidIO).

Understanding of the capabilities of the multicore platform is vital for successful experiments. In the OpenEM version of the workload the multicore communication will be handled using Multicore Navigator through the TI OpenEM implementation. The explicit Inter Processor Communication will be utilized in the simple Actor version of the workload.

\subsection{Code Composer Studio v5 / v6}
Code Composer Studio is Eclipse based IDE for developing for TI hardware. CCS features a wide variety of debug, trace and system analysis tools.

\subsection{OpenEM}
Open Event Machine (OpenEM) is a framework of an event driven multicore optimized processing originally developed at NSN. In this thesis OpenEM will be used as the platform for parallelism. First item in the work plan is to get familiar with OpenEM in general and the Texas Instruments OpenEM implementation. TI OpenEM 1.0.0.2 is the newest OpenEM implementation available. (if anyone finds anything newer let me know.)

The Texas Instruments OpenEM implementation is a parallel runtime providing inter core communication, event scheduling, event queues and other required components for parallel processing applications. OpenEM is Operating System independent runtime framework. The TI OpenEM white paper states the following ``OpenEM is able to operate in a heterogeneous OS environment: some cores may have SW running on the bare metal, others may have SW running on an OS and still others may have SW running on another OS.''

In OpenEM the general problem of locking in concurrent programming is transformed in to a scheduling problem. Whenever two threads need the same resource they are scheduled sequentially so that they don't try access the resource simultaneously.

When OpenEM schedules an event, it allocates the resources for that event and lets it run to completion. The next event utilizing the same resources might be something completely different depending on which events are ready for execution. There are no local buffers tied to certain memory locations, as all resource allocation and event execution is scheduled on the fly.
\subsection{People}
Vesa Hirvisalo - instructor

Jussi Hanhirova - sort of internal client / co-researcher

Kristian Hartikainen - research on similar subjects